{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRq0syKC5lvV"
   },
   "source": [
    "##  Let us begin\n",
    "\n",
    "Can add descriptions later for users to load interactively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdRF5Nr3zCQy",
    "outputId": "c3efc159-9669-480d-d9c4-10d05943550d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA RTX A6000 (UUID: GPU-69fba953-94a3-6b1e-c297-1c9b2d2713a9)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0LfvspJpN1x",
    "outputId": "9ee675d5-b1b1-4cb8-86f4-62ca4f06758c"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# !pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "q6oQVsXSLJW-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import sys\n",
    "import importlib as ipb\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import GPUtil\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VFMP5RgRYnhA"
   },
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "      \n",
    "    def pickle(self, key_save):\n",
    "        f = open(key_save, 'wb')\n",
    "        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "\n",
    "    def unpickle(self, key_save):\n",
    "        with open(key_save, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 2\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 0].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Test: {result[:, 1].max():.2f}')\n",
    "            # Same as highest train, as we have no validation data\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}') \n",
    "            print(f'   Final Test: {result[argmax, 1]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train = r[:, 0].max().item()\n",
    "                test = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 0].argmax(), 0].item()\n",
    "                test2 = r[r[:, 0].argmax(), 1].item()\n",
    "                best_results.append((train, test, train2, test2))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Test: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')\n",
    "\n",
    "\n",
    "def test(model, train_loader, test_loader):\n",
    "    model.eval()\n",
    "    loader = {0: train_loader, 1:test_loader}\n",
    "    accuracies = []\n",
    "    with torch.no_grad():\n",
    "        for data_loader in loader.values():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, (images, labels) in enumerate(data_loader):\n",
    "                if args.FC:\n",
    "                    images = images.reshape(-1, input_size).to(device)\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            accuracies.append(correct/total)\n",
    "    return accuracies # train_accu & test accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "se13vA7gjTeh"
   },
   "source": [
    "### Training function (include SVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Zvp-50gniy1w"
   },
   "outputs": [],
   "source": [
    "def train(model, images, labels):\n",
    "    model.train()\n",
    "    out = model(images)\n",
    "    cost = nn.CrossEntropyLoss()\n",
    "    loss = cost(out, labels)\n",
    "    # loss = F.mse_loss(out.float(), F.one_hot(labels).float())/2\n",
    "    loss.backward()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train_SVI(model, images, labels):\n",
    "    #### New lines for SVI ####\n",
    "    # NOTE: lines below are necessary, as o/w model.layers_x grow in size as epoches increases\n",
    "    model.layers_Xtilde = []\n",
    "    model.layers_grad = []\n",
    "    model.on_training = True\n",
    "    #### End #####\n",
    "    model.train()\n",
    "    out = model(images)\n",
    "    cost = nn.CrossEntropyLoss()\n",
    "    loss = cost(out, labels)\n",
    "    # loss = F.mse_loss(out.float(), F.one_hot(labels).float())/2\n",
    "    loss.backward(retain_graph=True)\n",
    "    #### New lines for SVI ####\n",
    "    loss_tilde = 0\n",
    "    for Xlplus1, Xlplus1grad in zip(reversed(model.layers_Xtilde), reversed(model.layers_grad)):\n",
    "        Xlplus1grad = Xlplus1grad.grad.detach().to(device)\n",
    "        loss_tilde += (Xlplus1*Xlplus1grad).sum()\n",
    "    loss_tilde.backward()  # To get update direction by MVI for all layers at once\n",
    "    model.on_training = False  # To avoid additional .retain_grad()\n",
    "    #### End #####\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ekr8mjqiXld"
   },
   "source": [
    "### FCNet (include SVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RjT-QXlmic0T"
   },
   "outputs": [],
   "source": [
    "class FCnet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(FCnet, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(nn.Linear(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(nn.Linear(hidden_channels, out_channels))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if i > 0:\n",
    "                x = self.bns[i-1](x)\n",
    "            x = conv(x)\n",
    "            if i < len(self.convs)-1:\n",
    "                x = F.relu(x)\n",
    "            if i == len(self.convs[:-1])-1:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x.softmax(dim=-1)\n",
    "\n",
    "\n",
    "class FCnet_SVI(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(FCnet_SVI, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(nn.Linear(in_channels, hidden_channels))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(nn.Linear(hidden_channels, hidden_channels))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        self.convs.append(nn.Linear(hidden_channels, out_channels))\n",
    "        self.dropout = dropout\n",
    "        #### New lines for SVI ####\n",
    "        # If any layer true, then it needs to use SVI\n",
    "        self.layers_with_SVI = [True for i in range(len(self.convs))]\n",
    "        # TODO: later, can treat this as an input to be decided (as some layers need not SVI)\n",
    "        # Append pre-activation \\tilde{X}_{l+1}, ONLY at layers i where self.layers_with_SVI[i] == True\n",
    "        self.layers_Xtilde = []\n",
    "        # Append the grad of L w.r.t. X_{l+1}, ONLY at layers i where self.layers_with_SVI[i] == True\n",
    "        self.layers_grad = []\n",
    "        self.on_training = True # If False, then we do not use SVI.\n",
    "        #### End #####\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if i > 0:\n",
    "                x = self.bns[i-1](x)\n",
    "            #### New lines for SVI ####\n",
    "            # This is added b/c o/w the gradient backprop w.r.t. new loss also gets to earlier layers (undesirable)\n",
    "            if self.layers_with_SVI[i] and self.on_training:\n",
    "                x_tmp = x.detach().clone().to(device)\n",
    "                self.layers_Xtilde.append(conv(x_tmp))\n",
    "            #### End #####\n",
    "            x = conv(x)\n",
    "            if i < len(self.convs)-1:\n",
    "                x = F.relu(x)\n",
    "            if i == len(self.convs[:-1])-1:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)     \n",
    "            #### New lines for SVI ####\n",
    "            if self.layers_with_SVI[i] and self.on_training:\n",
    "                x.retain_grad()  # To get the gradient with respect to output\n",
    "                self.layers_grad.append(x)\n",
    "            #### End #####\n",
    "        return x.softmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3cokjaGiaQm"
   },
   "source": [
    "### LeNet (include SVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jllY8RZfy0mf"
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    # From https://towardsdatascience.com/implementing-yann-lecuns-lenet-5-in-pytorch-5e05a0911320\n",
    "    def __init__(self, in_channels, n_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "\n",
    "        # The convolutional filters are feature extractors\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(nn.Conv2d(in_channels=in_channels,\n",
    "                          out_channels=6, kernel_size=5, stride=1))\n",
    "        self.convs.append(\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1))\n",
    "        self.convs.append(nn.Linear(in_features=400, out_features=120))\n",
    "        self.convs.append(nn.Linear(in_features=120, out_features=84))\n",
    "        self.convs.append(nn.Linear(in_features=84, out_features=n_classes))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm2d(6))\n",
    "        self.bns.append(torch.nn.BatchNorm2d(16))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if i == 2:\n",
    "                x = torch.flatten(x, 1)\n",
    "            x = conv(x)\n",
    "            if i < 2: \n",
    "                x = self.bns[i](x)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = nn.ReLU()(x)\n",
    "            if i < 2:\n",
    "                # NOTE: This X_{l+1} is current input before Average Pooling, as o/w the \\tilde{L} has dimension mismatch\n",
    "                x = nn.AvgPool2d(2, stride=2)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LeNet5_SVI(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes):\n",
    "        super(LeNet5_SVI, self).__init__()\n",
    "\n",
    "        # The convolutional filters are feature extractors\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(nn.Conv2d(in_channels=in_channels,\n",
    "                          out_channels=6, kernel_size=5, stride=1))\n",
    "        self.convs.append(\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1))\n",
    "        self.convs.append(nn.Linear(in_features=400, out_features=120))\n",
    "        self.convs.append(nn.Linear(in_features=120, out_features=84))\n",
    "        self.convs.append(nn.Linear(in_features=84, out_features=n_classes))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm2d(6))\n",
    "        self.bns.append(torch.nn.BatchNorm2d(16))\n",
    "\n",
    "        #### New lines for SVI ####\n",
    "        # If any layer true, then it needs to use SVI\n",
    "        self.layers_with_SVI = [True for i in range(len(self.convs))]\n",
    "        # TODO: later, can treat this as an input to be decided (as some layers need not SVI)\n",
    "        # Append pre-activation \\tilde{X}_{l+1}, ONLY at layers i where self.layers_with_SVI[i] == True\n",
    "        self.layers_Xtilde = []\n",
    "        # Append the grad of L w.r.t. X_{l+1}, ONLY at layers i where self.layers_with_SVI[i] == True\n",
    "        self.layers_grad = []\n",
    "        self.on_training = True\n",
    "        #### End #####\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if i == 2:\n",
    "                x = torch.flatten(x, 1)\n",
    "            #### New lines for SVI ####\n",
    "            # This is added b/c o/w the gradient backprop w.r.t. new loss also gets to earlier layers (undesirable)\n",
    "            if self.layers_with_SVI[i] and self.on_training:\n",
    "                x_tmp = x.detach().clone().to(device)\n",
    "                self.layers_Xtilde.append(conv(x_tmp))\n",
    "            #### End #####\n",
    "            x = conv(x)\n",
    "            if i < 2: \n",
    "                x = self.bns[i](x)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = nn.ReLU()(x)\n",
    "            #### New lines for SVI ####\n",
    "            if self.layers_with_SVI[i] and self.on_training:\n",
    "                x.retain_grad()  # To get the gradient with respect to output\n",
    "                self.layers_grad.append(x)\n",
    "            #### End #####\n",
    "            if i < 2:\n",
    "                # NOTE: This X_{l+1} is current input before Average Pooling, as o/w the \\tilde{L} has dimension mismatch\n",
    "                x = nn.AvgPool2d(2, stride=2)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGoC-5G_yxsU"
   },
   "source": [
    "### Utility function, including data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "q92j2DdFsuq9"
   },
   "outputs": [],
   "source": [
    "def mem_report():\n",
    "    if device.type == 'cuda':\n",
    "        GPUs = GPUtil.getGPUs()\n",
    "        for i, gpu in enumerate(GPUs):\n",
    "            print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(\n",
    "                i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
    "    else:\n",
    "        print(\"CPU RAM Free: \"\n",
    "              + humanize.naturalsize(psutil.virtual_memory().available))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "cc8e20eb10034e6d83571dbb4de5e577",
      "5a46479b03fc498db0a7ec8c1769a23c",
      "ec822baea4d54ffe9ea220b36f6bc9f7",
      "248ae6c24ca2412ea996878f22dc86ee",
      "5d6514d10dbd4dfbbf5e39c7bb509a61",
      "07130a8da9604dd99e939667923267b3",
      "20c8b7f462bd4076bca5d00b54dd2f25",
      "a186b0b2fb2b4142be4b088de5e30215",
      "5d0a979aba644f4f83381b4fafecc257",
      "fd6d3e06c2bb44c889065787b9389dc6",
      "52c8df0956304649880aced8b60a415f"
     ]
    },
    "id": "ertPh34Q0yYA",
    "outputId": "c35c2428-0620-45d4-b28b-cd1da5e2cbe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "def subset_data(data,frac=1):\n",
    "    # Randomly subset a fraction of data from total data\n",
    "    np.random.seed(1103)\n",
    "    idx = np.random.choice(len(data),int(frac*len(data)),replace=False)\n",
    "    return torch.utils.data.Subset(data,idx)\n",
    "data_fixed = 'MNIST_batched' # 'MNIST_batched' or 'CIFAR10_batched'\n",
    "if 'MNIST' in data_fixed:\n",
    "    train_dataset0 = torchvision.datasets.MNIST(root = './data',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "    test_dataset0 = torchvision.datasets.MNIST(root = './data',\n",
    "                                              train = False,\n",
    "                                              transform = transforms.Compose([\n",
    "                                                      transforms.Resize((32,32)),\n",
    "                                                      transforms.ToTensor(),\n",
    "                                                      transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                              download=True)\n",
    "    num_classes = 10\n",
    "    in_channels = 1\n",
    "else:\n",
    "    # NOTE, CIFAR10 has color channels, so input size = 3*32*32 with FC net\n",
    "    train_dataset0 = torchvision.datasets.CIFAR10(root='/data', \n",
    "                                              train=True, \n",
    "                                              transform=transforms.ToTensor(),  \n",
    "                                              download=True)\n",
    "    test_dataset0 = torchvision.datasets.CIFAR10(root='/data', \n",
    "                                              train=False, \n",
    "                                              transform=transforms.ToTensor())\n",
    "    num_classes = 10 \n",
    "    in_channels = 3\n",
    "input_size = torch.prod(torch.tensor(train_dataset0[0][0].shape)).item()\n",
    "frac=0.1 if 'MNIST' in data_fixed else 0.2\n",
    "frac = 1\n",
    "train_dataset = subset_data(train_dataset0,frac=frac)\n",
    "test_dataset = subset_data(test_dataset0,frac=frac)\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "# Data loader (i.e., split to batches) see below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqAIlL3WUylQ"
   },
   "source": [
    "Start testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(log_steps=468, num_layers=4, dropout=0.25, lr=0.001, momentum=0.95, epochs=20, batch_size=128, runs=3, SVI=True, FC=False, optimizer='SGD', hidden_channels=512)\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 1\n",
      "Run: 01, Epoch: 01, Loss: 0.1810, Train: 95.10%,Test: 95.27%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 2\n",
      "Run: 01, Epoch: 02, Loss: 0.1699, Train: 97.25%,Test: 97.33%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 3\n",
      "Run: 01, Epoch: 03, Loss: 0.0495, Train: 97.35%,Test: 97.74%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 4\n",
      "Run: 01, Epoch: 04, Loss: 0.1093, Train: 97.85%,Test: 98.09%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 5\n",
      "Run: 01, Epoch: 05, Loss: 0.0689, Train: 98.17%,Test: 98.38%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 6\n",
      "Run: 01, Epoch: 06, Loss: 0.0323, Train: 98.28%,Test: 98.50%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 7\n",
      "Run: 01, Epoch: 07, Loss: 0.0705, Train: 98.50%,Test: 98.62%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 8\n",
      "Run: 01, Epoch: 08, Loss: 0.0202, Train: 98.80%,Test: 98.64%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 9\n",
      "Run: 01, Epoch: 09, Loss: 0.0207, Train: 98.70%,Test: 98.74%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 10\n",
      "Run: 01, Epoch: 10, Loss: 0.0164, Train: 98.75%,Test: 98.63%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 11\n",
      "Run: 01, Epoch: 11, Loss: 0.0496, Train: 98.93%,Test: 98.84%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 12\n",
      "Run: 01, Epoch: 12, Loss: 0.0101, Train: 98.75%,Test: 98.77%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 13\n",
      "Run: 01, Epoch: 13, Loss: 0.0285, Train: 99.03%,Test: 98.85%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 14\n",
      "Run: 01, Epoch: 14, Loss: 0.0298, Train: 99.05%,Test: 98.94%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 15\n",
      "Run: 01, Epoch: 15, Loss: 0.0276, Train: 99.12%,Test: 98.96%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 16\n",
      "Run: 01, Epoch: 16, Loss: 0.0189, Train: 99.03%,Test: 98.79%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 17\n",
      "Run: 01, Epoch: 17, Loss: 0.0167, Train: 99.27%,Test: 99.08%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 18\n",
      "Run: 01, Epoch: 18, Loss: 0.0071, Train: 99.20%,Test: 99.04%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 19\n",
      "Run: 01, Epoch: 19, Loss: 0.0146, Train: 99.25%,Test: 99.00%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 20\n",
      "Run: 01, Epoch: 20, Loss: 0.0330, Train: 99.27%,Test: 99.00%\n",
      "Run 01:\n",
      "Highest Train: 99.27\n",
      "Highest Test: 99.08\n",
      "  Final Train: 99.27\n",
      "   Final Test: 99.08\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 1\n",
      "Run: 02, Epoch: 01, Loss: 0.2637, Train: 93.23%,Test: 93.88%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 2\n",
      "Run: 02, Epoch: 02, Loss: 0.1214, Train: 95.97%,Test: 96.42%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 3\n",
      "Run: 02, Epoch: 03, Loss: 0.1036, Train: 96.98%,Test: 97.34%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 4\n",
      "Run: 02, Epoch: 04, Loss: 0.0380, Train: 97.33%,Test: 97.88%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 5\n",
      "Run: 02, Epoch: 05, Loss: 0.0526, Train: 97.73%,Test: 97.99%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 6\n",
      "Run: 02, Epoch: 06, Loss: 0.0367, Train: 97.38%,Test: 98.01%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 7\n",
      "Run: 02, Epoch: 07, Loss: 0.0569, Train: 98.10%,Test: 98.28%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 8\n",
      "Run: 02, Epoch: 08, Loss: 0.0183, Train: 98.17%,Test: 98.42%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 9\n",
      "Run: 02, Epoch: 09, Loss: 0.0398, Train: 98.65%,Test: 98.83%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 10\n",
      "Run: 02, Epoch: 10, Loss: 0.0507, Train: 98.53%,Test: 98.80%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 11\n",
      "Run: 02, Epoch: 11, Loss: 0.0562, Train: 98.57%,Test: 98.79%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 12\n",
      "Run: 02, Epoch: 12, Loss: 0.0626, Train: 98.62%,Test: 98.80%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 13\n",
      "Run: 02, Epoch: 13, Loss: 0.0302, Train: 98.45%,Test: 98.76%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 14\n",
      "Run: 02, Epoch: 14, Loss: 0.0245, Train: 98.40%,Test: 98.59%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 15\n",
      "Run: 02, Epoch: 15, Loss: 0.0093, Train: 98.87%,Test: 98.94%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 16\n",
      "Run: 02, Epoch: 16, Loss: 0.0100, Train: 99.08%,Test: 99.00%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 17\n",
      "Run: 02, Epoch: 17, Loss: 0.0624, Train: 98.88%,Test: 98.96%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 18\n",
      "Run: 02, Epoch: 18, Loss: 0.0305, Train: 99.03%,Test: 99.15%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 19\n",
      "Run: 02, Epoch: 19, Loss: 0.0549, Train: 99.10%,Test: 99.02%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 20\n",
      "Run: 02, Epoch: 20, Loss: 0.0573, Train: 99.18%,Test: 99.18%\n",
      "Run 02:\n",
      "Highest Train: 99.18\n",
      "Highest Test: 99.18\n",
      "  Final Train: 99.18\n",
      "   Final Test: 99.18\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 1\n",
      "Run: 03, Epoch: 01, Loss: 0.1423, Train: 95.07%,Test: 95.46%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 2\n",
      "Run: 03, Epoch: 02, Loss: 0.1339, Train: 96.85%,Test: 97.20%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 3\n",
      "Run: 03, Epoch: 03, Loss: 0.1021, Train: 96.62%,Test: 96.97%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 4\n",
      "Run: 03, Epoch: 04, Loss: 0.0810, Train: 97.82%,Test: 98.16%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 5\n",
      "Run: 03, Epoch: 05, Loss: 0.0352, Train: 98.00%,Test: 98.34%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 6\n",
      "Run: 03, Epoch: 06, Loss: 0.0560, Train: 98.33%,Test: 98.53%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 7\n",
      "Run: 03, Epoch: 07, Loss: 0.0436, Train: 98.43%,Test: 98.56%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 8\n",
      "Run: 03, Epoch: 08, Loss: 0.0758, Train: 98.58%,Test: 98.68%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 9\n",
      "Run: 03, Epoch: 09, Loss: 0.0453, Train: 98.75%,Test: 98.88%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 10\n",
      "Run: 03, Epoch: 10, Loss: 0.0189, Train: 98.55%,Test: 98.55%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 11\n",
      "Run: 03, Epoch: 11, Loss: 0.0822, Train: 98.88%,Test: 98.85%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 12\n",
      "Run: 03, Epoch: 12, Loss: 0.0185, Train: 99.07%,Test: 98.74%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 13\n",
      "Run: 03, Epoch: 13, Loss: 0.0064, Train: 98.97%,Test: 98.87%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 14\n",
      "Run: 03, Epoch: 14, Loss: 0.0720, Train: 98.98%,Test: 98.89%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 15\n",
      "Run: 03, Epoch: 15, Loss: 0.0175, Train: 99.18%,Test: 98.96%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 16\n",
      "Run: 03, Epoch: 16, Loss: 0.0396, Train: 99.23%,Test: 98.89%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 17\n",
      "Run: 03, Epoch: 17, Loss: 0.0065, Train: 99.25%,Test: 98.92%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 18\n",
      "Run: 03, Epoch: 18, Loss: 0.0195, Train: 99.32%,Test: 99.00%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 19\n",
      "Run: 03, Epoch: 19, Loss: 0.0190, Train: 98.72%,Test: 98.59%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 467, epoch 20\n",
      "Run: 03, Epoch: 20, Loss: 0.0331, Train: 99.25%,Test: 99.00%\n",
      "Run 03:\n",
      "Highest Train: 99.32\n",
      "Highest Test: 99.00\n",
      "  Final Train: 99.32\n",
      "   Final Test: 99.00\n",
      "All runs:\n",
      "Highest Train: 99.26 ± 0.07\n",
      "Highest Test: 99.09 ± 0.09\n",
      "  Final Train: 99.26 ± 0.07\n",
      "   Final Test: 99.09 ± 0.09\n",
      "Namespace(log_steps=468, num_layers=4, dropout=0.25, lr=0.001, momentum=0.95, epochs=20, batch_size=128, runs=3, SVI=True, FC=False, optimizer='Adam', hidden_channels=512)\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 1\n",
      "Run: 01, Epoch: 01, Loss: 0.0912, Train: 98.00%,Test: 98.06%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 2\n",
      "Run: 01, Epoch: 02, Loss: 0.0430, Train: 98.33%,Test: 98.18%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 3\n",
      "Run: 01, Epoch: 03, Loss: 0.0188, Train: 98.43%,Test: 98.29%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 4\n",
      "Run: 01, Epoch: 04, Loss: 0.0427, Train: 99.12%,Test: 99.10%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 5\n",
      "Run: 01, Epoch: 05, Loss: 0.1171, Train: 99.15%,Test: 98.73%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 6\n",
      "Run: 01, Epoch: 06, Loss: 0.0048, Train: 99.17%,Test: 98.93%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 7\n",
      "Run: 01, Epoch: 07, Loss: 0.0292, Train: 99.30%,Test: 99.15%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 8\n",
      "Run: 01, Epoch: 08, Loss: 0.0031, Train: 99.02%,Test: 99.02%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 9\n",
      "Run: 01, Epoch: 09, Loss: 0.0046, Train: 99.48%,Test: 99.19%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 10\n",
      "Run: 01, Epoch: 10, Loss: 0.0039, Train: 99.15%,Test: 98.85%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 11\n",
      "Run: 01, Epoch: 11, Loss: 0.0348, Train: 99.63%,Test: 99.20%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 12\n",
      "Run: 01, Epoch: 12, Loss: 0.0027, Train: 99.40%,Test: 99.04%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 13\n",
      "Run: 01, Epoch: 13, Loss: 0.0047, Train: 99.63%,Test: 99.11%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 14\n",
      "Run: 01, Epoch: 14, Loss: 0.0149, Train: 99.43%,Test: 99.08%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 15\n",
      "Run: 01, Epoch: 15, Loss: 0.0241, Train: 98.90%,Test: 98.58%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 16\n",
      "Run: 01, Epoch: 16, Loss: 0.0075, Train: 99.50%,Test: 99.01%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 17\n",
      "Run: 01, Epoch: 17, Loss: 0.0010, Train: 99.55%,Test: 99.05%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 18\n",
      "Run: 01, Epoch: 18, Loss: 0.0018, Train: 99.32%,Test: 99.01%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 19\n",
      "Run: 01, Epoch: 19, Loss: 0.0007, Train: 99.23%,Test: 98.83%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 20\n",
      "Run: 01, Epoch: 20, Loss: 0.0299, Train: 99.42%,Test: 99.02%\n",
      "Run 01:\n",
      "Highest Train: 99.63\n",
      "Highest Test: 99.20\n",
      "  Final Train: 99.63\n",
      "   Final Test: 99.20\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 1\n",
      "Run: 02, Epoch: 01, Loss: 0.1170, Train: 97.48%,Test: 97.81%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 2\n",
      "Run: 02, Epoch: 02, Loss: 0.1055, Train: 98.50%,Test: 98.49%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 3\n",
      "Run: 02, Epoch: 03, Loss: 0.0448, Train: 98.95%,Test: 98.69%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 4\n",
      "Run: 02, Epoch: 04, Loss: 0.0247, Train: 98.45%,Test: 98.49%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 5\n",
      "Run: 02, Epoch: 05, Loss: 0.0217, Train: 98.63%,Test: 98.54%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 6\n",
      "Run: 02, Epoch: 06, Loss: 0.0257, Train: 99.23%,Test: 99.03%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 7\n",
      "Run: 02, Epoch: 07, Loss: 0.0257, Train: 98.80%,Test: 98.57%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 8\n",
      "Run: 02, Epoch: 08, Loss: 0.0039, Train: 99.28%,Test: 98.79%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 9\n",
      "Run: 02, Epoch: 09, Loss: 0.0164, Train: 99.42%,Test: 99.05%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 10\n",
      "Run: 02, Epoch: 10, Loss: 0.0306, Train: 99.48%,Test: 98.95%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 11\n",
      "Run: 02, Epoch: 11, Loss: 0.0255, Train: 98.95%,Test: 98.57%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 12\n",
      "Run: 02, Epoch: 12, Loss: 0.0226, Train: 99.57%,Test: 99.05%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 13\n",
      "Run: 02, Epoch: 13, Loss: 0.0273, Train: 99.47%,Test: 99.05%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 14\n",
      "Run: 02, Epoch: 14, Loss: 0.0080, Train: 99.55%,Test: 98.99%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 15\n",
      "Run: 02, Epoch: 15, Loss: 0.0103, Train: 99.43%,Test: 98.95%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 16\n",
      "Run: 02, Epoch: 16, Loss: 0.0149, Train: 98.95%,Test: 98.54%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 17\n",
      "Run: 02, Epoch: 17, Loss: 0.0157, Train: 99.57%,Test: 98.99%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 18\n",
      "Run: 02, Epoch: 18, Loss: 0.0099, Train: 98.82%,Test: 98.52%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 19\n",
      "Run: 02, Epoch: 19, Loss: 0.0053, Train: 99.65%,Test: 98.96%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 20\n",
      "Run: 02, Epoch: 20, Loss: 0.0191, Train: 99.73%,Test: 99.26%\n",
      "Run 02:\n",
      "Highest Train: 99.73\n",
      "Highest Test: 99.26\n",
      "  Final Train: 99.73\n",
      "   Final Test: 99.26\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 1\n",
      "Run: 03, Epoch: 01, Loss: 0.0592, Train: 97.83%,Test: 97.80%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 2\n",
      "Run: 03, Epoch: 02, Loss: 0.0581, Train: 98.57%,Test: 98.50%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 3\n",
      "Run: 03, Epoch: 03, Loss: 0.0379, Train: 98.82%,Test: 98.62%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 4\n",
      "Run: 03, Epoch: 04, Loss: 0.0548, Train: 98.85%,Test: 98.76%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 5\n",
      "Run: 03, Epoch: 05, Loss: 0.0127, Train: 98.98%,Test: 98.93%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 6\n",
      "Run: 03, Epoch: 06, Loss: 0.0552, Train: 97.42%,Test: 97.55%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 7\n",
      "Run: 03, Epoch: 07, Loss: 0.0083, Train: 99.37%,Test: 99.11%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 8\n",
      "Run: 03, Epoch: 08, Loss: 0.0875, Train: 99.28%,Test: 98.99%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 9\n",
      "Run: 03, Epoch: 09, Loss: 0.0044, Train: 98.95%,Test: 98.70%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 10\n",
      "Run: 03, Epoch: 10, Loss: 0.0048, Train: 99.05%,Test: 98.70%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 11\n",
      "Run: 03, Epoch: 11, Loss: 0.0128, Train: 99.48%,Test: 99.07%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 12\n",
      "Run: 03, Epoch: 12, Loss: 0.0086, Train: 99.30%,Test: 98.78%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 13\n",
      "Run: 03, Epoch: 13, Loss: 0.0025, Train: 98.72%,Test: 98.28%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 14\n",
      "Run: 03, Epoch: 14, Loss: 0.0084, Train: 99.43%,Test: 99.00%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 15\n",
      "Run: 03, Epoch: 15, Loss: 0.0095, Train: 99.03%,Test: 98.76%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 16\n",
      "Run: 03, Epoch: 16, Loss: 0.0302, Train: 99.40%,Test: 99.12%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 17\n",
      "Run: 03, Epoch: 17, Loss: 0.0017, Train: 99.32%,Test: 98.83%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 18\n",
      "Run: 03, Epoch: 18, Loss: 0.0035, Train: 99.43%,Test: 98.90%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 19\n",
      "Run: 03, Epoch: 19, Loss: 0.0011, Train: 99.67%,Test: 99.11%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-Adam training at batch 467, epoch 20\n",
      "Run: 03, Epoch: 20, Loss: 0.0137, Train: 99.40%,Test: 98.85%\n",
      "Run 03:\n",
      "Highest Train: 99.67\n",
      "Highest Test: 99.12\n",
      "  Final Train: 99.67\n",
      "   Final Test: 99.11\n",
      "All runs:\n",
      "Highest Train: 99.68 ± 0.05\n",
      "Highest Test: 99.19 ± 0.07\n",
      "  Final Train: 99.68 ± 0.05\n",
      "   Final Test: 99.19 ± 0.08\n"
     ]
    }
   ],
   "source": [
    "SVI_ls = [True, True, False]  # If False, use ordinary SGD or Adam\n",
    "SVI_pause = [False, True, False]  # If True, only SVI to warm start\n",
    "SVI_ls = [True]\n",
    "SVI_pause = [True]\n",
    "SVI_ls = [True]\n",
    "SVI_pause = [False]\n",
    "optim_ls = ['SGD', 'Adam']\n",
    "lr, hidden_channels = 0.001, 512\n",
    "FC_only = False  # Use all FC layers if True, o/w use LeNet\n",
    "dataname = data_fixed\n",
    "if FC_only == False:\n",
    "    dataname = data_fixed + 'LeNet_'\n",
    "num_runs = 3\n",
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "num_log_steps = len(train_dataset) // batch_size # Evaluate loss every X iteration/mini-batches\n",
    "if __name__ == \"__main__\":\n",
    "    Final_result = {}  # For quick check at the end of training\n",
    "    for use_SVI, pause_SVI in zip(SVI_ls, SVI_pause):\n",
    "        for optim_name in optim_ls:\n",
    "            result_dict = {'SVI-SGD': [],\n",
    "                           'SVI_warmstart-SGD': [],\n",
    "                           'SGD': [],\n",
    "                           'SVI-Adam': [],\n",
    "                           'SVI_warmstart-Adam': [],\n",
    "                           'Adam': []}\n",
    "            parser = argparse.ArgumentParser(\n",
    "                description=dataname)\n",
    "            parser.add_argument('--log_steps', type=int, default=num_log_steps)\n",
    "            parser.add_argument('--num_layers', type=int, default=4)\n",
    "            parser.add_argument('--dropout', type=float, default=0.25)\n",
    "            parser.add_argument('--lr', type=float, default=lr)\n",
    "            parser.add_argument('--momentum', type=float, default=0.95)\n",
    "            parser.add_argument(\n",
    "                '--epochs', type=int, default=num_epochs)  # Change to 100\n",
    "            parser.add_argument(\n",
    "                '--batch_size', type=int, default=batch_size)\n",
    "            parser.add_argument('--runs', type=int,\n",
    "                                default=num_runs)\n",
    "            parser.add_argument('--SVI', type=bool, default=use_SVI)\n",
    "            parser.add_argument('--FC', type=bool, default=FC_only)\n",
    "            parser.add_argument(\n",
    "                '--optimizer', type=str, default=optim_name)\n",
    "            args = parser.parse_args(args=[])\n",
    "            args.hidden_channels = hidden_channels\n",
    "            print(args)\n",
    "            # Get data loader from dataset\n",
    "            train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       shuffle=True)\n",
    "            test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      shuffle=False)\n",
    "            logger = Logger(args.runs, args)\n",
    "            results_over_runs = {}\n",
    "            for run in range(args.runs):\n",
    "                accu_at_run = []\n",
    "                # Initiatiate new model per run\n",
    "                torch.manual_seed(1103 + run)\n",
    "                args.SVI = use_SVI\n",
    "                if args.SVI:\n",
    "                    if args.FC:\n",
    "                        model = FCnet_SVI(input_size, args.hidden_channels,\n",
    "                                          num_classes, args.num_layers, args.dropout).to(device)\n",
    "                    else:\n",
    "                        model = LeNet5_SVI(in_channels, num_classes).to(device)\n",
    "                else:\n",
    "                    if args.FC:\n",
    "                        model = FCnet(input_size, args.hidden_channels,\n",
    "                                      num_classes, args.num_layers, args.dropout).to(device)\n",
    "                    else:\n",
    "                        model = LeNet5(in_channels, num_classes).to(device)\n",
    "                if args.optimizer == 'SGD':\n",
    "                    optimizer = torch.optim.SGD(\n",
    "                        model.parameters(), lr=args.lr, momentum=args.momentum, nesterov=True)\n",
    "                else:\n",
    "                    optimizer = torch.optim.Adam(\n",
    "                        model.parameters(), lr=args.lr)\n",
    "                for epoch in range(1, 1 + args.epochs):\n",
    "                    iter_per_epoch = int(len(train_dataset) / batch_size) + 1\n",
    "                    frac_stop = 0.1\n",
    "                    pause_threshold = int(\n",
    "                        frac_stop * iter_per_epoch * args.epochs)\n",
    "                    if device.type == 'cuda':\n",
    "                        # Useful to avoid GPU allocation excess\n",
    "                        torch.cuda.empty_cache()\n",
    "                    print(f\"LR is {optimizer.param_groups[0]['lr']}\")\n",
    "                    for i, (images, labels) in enumerate(train_loader):\n",
    "                        current_itr = ((epoch - 1) * iter_per_epoch) + i + 1\n",
    "                        if pause_SVI and current_itr > pause_threshold:\n",
    "                            args.SVI = False\n",
    "                            if current_itr <= pause_threshold + 1:\n",
    "                                # Reinitialize optimizer to avoid gradient issue in Adam\n",
    "                                sdict = model.state_dict()\n",
    "                                print(\n",
    "                                    '############ Pause SVI from now on ############')\n",
    "                                if args.FC:\n",
    "                                    model = FCnet(input_size, args.hidden_channels,\n",
    "                                                  num_classes, args.num_layers, args.dropout)\n",
    "                                else:\n",
    "                                    model = LeNet5(in_channels, num_classes)\n",
    "                                model.load_state_dict(sdict)\n",
    "                                model = model.to(device)\n",
    "                                optimizer = torch.optim.Adam(\n",
    "                                    model.parameters(), lr=args.lr)\n",
    "                        optimizer.zero_grad()\n",
    "                        if args.FC:\n",
    "                            images = images.reshape(-1, input_size).to(device)\n",
    "                        images = images.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        loss = train_SVI(model, images, labels) if args.SVI else train(model, images, labels)\n",
    "                        optimizer.step()\n",
    "                        # Test at each iteration\n",
    "                        if (i+1) % args.log_steps == 0:\n",
    "                            print('Testing')\n",
    "                            if args.SVI:\n",
    "                                print(f'SVI-{args.optimizer} training at batch {i}, epoch {epoch}')\n",
    "                            else:\n",
    "                                print(f'{args.optimizer} training at batch {i}, epoch {epoch}')\n",
    "                            # Do so because training data has too many images, and evaluation thus takes too long.\n",
    "                            train_loader_sub = torch.utils.data.DataLoader(dataset=subset_data(train_dataset0,frac=0.1),\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       shuffle=True)\n",
    "                            result = test(model, train_loader_sub, test_loader)\n",
    "                            logger.add_result(run, result)\n",
    "                            train_acc, test_acc = result\n",
    "                            accu_at_run += [[train_acc, test_acc]]\n",
    "                            print(f'Run: {run + 1:02d}, '\n",
    "                                  f'Epoch: {epoch:02d}, '\n",
    "                                  f'Loss: {loss:.4f}, '\n",
    "                                  f'Train: {100 * train_acc:.2f}%,'\n",
    "                                  f'Test: {100 * test_acc:.2f}%')\n",
    "                # Save model after training over all epochs\n",
    "                results_over_runs[f'lr={args.lr}@Run{run+1}'] = accu_at_run\n",
    "                logger.print_statistics(run)\n",
    "                # Save results\n",
    "                if use_SVI:\n",
    "                    SVI_prefix = 'SVI_warmstart-' if pause_SVI else 'SVI-'\n",
    "                else:\n",
    "                    SVI_prefix = ''\n",
    "                key = f'{SVI_prefix}{optim_name}'\n",
    "                key_save = f'{SVI_prefix}{optim_name}-{args.num_layers}layers-{args.hidden_channels}nodes-{args.lr}LR'\n",
    "                key_save = dataname + key_save\n",
    "                # Save it to file, but need not now because only one run.\n",
    "                logger.pickle(key_save)\n",
    "                result_dict[key].append(results_over_runs)\n",
    "                # Final train and test accuracy\n",
    "                Final_result[key] = accu_at_run[-1]\n",
    "                with open(f\"{key_save}_loss_together.json\", \"w\") as outfile:\n",
    "                    json.dump(result_dict, outfile)\n",
    "            logger.print_statistics()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8Ekr8mjqiXld",
    "DJp7BXpnP-5Q",
    "e-jvjKsmxOZO"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07130a8da9604dd99e939667923267b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20c8b7f462bd4076bca5d00b54dd2f25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "248ae6c24ca2412ea996878f22dc86ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd6d3e06c2bb44c889065787b9389dc6",
      "placeholder": "​",
      "style": "IPY_MODEL_52c8df0956304649880aced8b60a415f",
      "value": " 170498071/170498071 [00:03&lt;00:00, 54226204.17it/s]"
     }
    },
    "52c8df0956304649880aced8b60a415f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a46479b03fc498db0a7ec8c1769a23c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07130a8da9604dd99e939667923267b3",
      "placeholder": "​",
      "style": "IPY_MODEL_20c8b7f462bd4076bca5d00b54dd2f25",
      "value": "100%"
     }
    },
    "5d0a979aba644f4f83381b4fafecc257": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d6514d10dbd4dfbbf5e39c7bb509a61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a186b0b2fb2b4142be4b088de5e30215": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc8e20eb10034e6d83571dbb4de5e577": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a46479b03fc498db0a7ec8c1769a23c",
       "IPY_MODEL_ec822baea4d54ffe9ea220b36f6bc9f7",
       "IPY_MODEL_248ae6c24ca2412ea996878f22dc86ee"
      ],
      "layout": "IPY_MODEL_5d6514d10dbd4dfbbf5e39c7bb509a61"
     }
    },
    "ec822baea4d54ffe9ea220b36f6bc9f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a186b0b2fb2b4142be4b088de5e30215",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d0a979aba644f4f83381b4fafecc257",
      "value": 170498071
     }
    },
    "fd6d3e06c2bb44c889065787b9389dc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
