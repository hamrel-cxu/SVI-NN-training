{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRq0syKC5lvV"
   },
   "source": [
    "##  Let us begin\n",
    "\n",
    "Can add descriptions later for users to load interactively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdRF5Nr3zCQy",
    "outputId": "c3efc159-9669-480d-d9c4-10d05943550d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA RTX A6000 (UUID: GPU-8a75c7fd-6edd-bf0e-2d95-54106b2b6ada)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0LfvspJpN1x",
    "outputId": "9ee675d5-b1b1-4cb8-86f4-62ca4f06758c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GPUtil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: GPUtil\n",
      "  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7394 sha256=a43ed625e9ca747d311829fe8ce7207ce22a077bd656ce27387e76ebdf27e2ad\n",
      "  Stored in directory: /root/.cache/pip/wheels/2b/b5/24/fbb56595c286984f7315ee31821d6121e1b9828436021a88b3\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: GPUtil\n",
      "Successfully installed GPUtil-1.4.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# !pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "q6oQVsXSLJW-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import sys\n",
    "import importlib as ipb\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import GPUtil\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VFMP5RgRYnhA"
   },
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "      \n",
    "    def pickle(self, key_save):\n",
    "        f = open(key_save, 'wb')\n",
    "        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "\n",
    "    def unpickle(self, key_save):\n",
    "        with open(key_save, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 2\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 0].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Test: {result[:, 1].max():.2f}')\n",
    "            # Same as highest train, as we have no validation data\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}') \n",
    "            print(f'   Final Test: {result[argmax, 1]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train = r[:, 0].max().item()\n",
    "                test = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 0].argmax(), 0].item()\n",
    "                test2 = r[r[:, 0].argmax(), 1].item()\n",
    "                best_results.append((train, test, train2, test2))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Test: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')\n",
    "\n",
    "\n",
    "def test(model, train_loader, test_loader):\n",
    "    model.eval()\n",
    "    loader = {0: train_loader, 1:test_loader}\n",
    "    accuracies = []\n",
    "    with torch.no_grad():\n",
    "        for data_loader in loader.values():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, (images, labels) in enumerate(data_loader):\n",
    "                if args.FC:\n",
    "                    images = images.reshape(-1, input_size).to(device)\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            accuracies.append(correct/total)\n",
    "    return accuracies # train_accu & test accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "se13vA7gjTeh"
   },
   "source": [
    "### Training function (include SVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Zvp-50gniy1w"
   },
   "outputs": [],
   "source": [
    "def train(model, images, labels):\n",
    "    model.train()\n",
    "    out = model(images)\n",
    "    cost = nn.CrossEntropyLoss()\n",
    "    loss = cost(out, labels)\n",
    "    # loss = F.mse_loss(out.float(), F.one_hot(labels).float())/2\n",
    "    loss.backward()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train_SVI(model, images, labels):\n",
    "    #### New lines for SVI ####\n",
    "    # NOTE: lines below are necessary, as o/w model.layers_x grow in size as epoches increases\n",
    "    model.layers_Xtilde = []\n",
    "    model.layers_grad = []\n",
    "    model.on_training = True\n",
    "    #### End #####\n",
    "    model.train()\n",
    "    out = model(images)\n",
    "    cost = nn.CrossEntropyLoss()\n",
    "    loss = cost(out, labels)\n",
    "    # loss = F.mse_loss(out.float(), F.one_hot(labels).float())/2\n",
    "    #### New lines for SVI ####\n",
    "    model.turn_on_off_grad(on = False) \n",
    "    # print('#### Grad of model params before SVI')\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.grad is not None:\n",
    "    #         print(name, param.grad.norm())\n",
    "    #### End #####\n",
    "    loss.backward()\n",
    "    #### New lines for SVI ####\n",
    "    loss_tilde = 0\n",
    "    for Xlplus1, Xlplus1grad in zip(model.layers_Xtilde, model.layers_grad):\n",
    "        Xlplus1grad = Xlplus1grad.grad.detach().to(device)\n",
    "        loss_tilde += (Xlplus1*Xlplus1grad).sum()\n",
    "    model.turn_on_off_grad(on = True)\n",
    "    loss_tilde.backward()  # To get update direction by MVI for all layers at once\n",
    "    # print('#### Grad of model params after SVI')\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.grad is not None:\n",
    "    #         print(name, param.grad.norm())\n",
    "    # raise Exception('Stop here')\n",
    "    model.on_training = False  # To avoid additional .retain_grad()\n",
    "    #### End #####\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3cokjaGiaQm"
   },
   "source": [
    "### LeNet (include SVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jllY8RZfy0mf"
   },
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    # From https://towardsdatascience.com/implementing-yann-lecuns-lenet-5-in-pytorch-5e05a0911320\n",
    "    def __init__(self, in_channels, n_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "\n",
    "        # The convolutional filters are feature extractors\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(nn.Conv2d(in_channels=in_channels,\n",
    "                          out_channels=6, kernel_size=5, stride=1))\n",
    "        self.convs.append(\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1))\n",
    "        self.convs.append(nn.Linear(in_features=400, out_features=120))\n",
    "        self.convs.append(nn.Linear(in_features=120, out_features=84))\n",
    "        self.convs.append(nn.Linear(in_features=84, out_features=n_classes))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm2d(6))\n",
    "        self.bns.append(torch.nn.BatchNorm2d(16))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if 0 < i <= 2:\n",
    "                x = nn.AvgPool2d(2, stride=2)(x)\n",
    "            if i == 2:\n",
    "                x = torch.flatten(x, 1)\n",
    "            x = conv(x)\n",
    "            if i < 2: \n",
    "                x = self.bns[i](x)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = nn.ReLU()(x) \n",
    "        return x\n",
    "\n",
    "\n",
    "class LeNet5_SVI(nn.Module):\n",
    "    def __init__(self, in_channels, n_classes):\n",
    "        super(LeNet5_SVI, self).__init__()\n",
    "\n",
    "        # The convolutional filters are feature extractors\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(nn.Conv2d(in_channels=in_channels,\n",
    "                          out_channels=6, kernel_size=5, stride=1))\n",
    "        self.convs.append(\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1))\n",
    "        self.convs.append(nn.Linear(in_features=400, out_features=120))\n",
    "        self.convs.append(nn.Linear(in_features=120, out_features=84))\n",
    "        self.convs.append(nn.Linear(in_features=84, out_features=n_classes))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm2d(6))\n",
    "        self.bns.append(torch.nn.BatchNorm2d(16))\n",
    "\n",
    "        #### New lines for SVI ####\n",
    "        # If any layer true, then it needs to use SVI\n",
    "        self.layers_with_SVI = [True for i in range(len(self.convs))]\n",
    "        # TODO: later, can treat this as an input to be decided (as some layers need not SVI)\n",
    "        # Append pre-activation \\tilde{X}_{l+1}, ONLY at layers i where self.layers_with_SVI[i] == True\n",
    "        self.layers_Xtilde = []\n",
    "        # Append the grad of L w.r.t. X_{l+1}, ONLY at layers i where self.layers_with_SVI[i] == True\n",
    "        self.layers_grad = []\n",
    "        self.on_training = True\n",
    "        #### End #####\n",
    "    \n",
    "    #### New lines for SVI ####\n",
    "    # Avoid gradient accumulation\n",
    "    def turn_on_off_grad(self, on = True):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = on\n",
    "            \n",
    "    #### End #####\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if 0 < i <= 2:\n",
    "                x = nn.AvgPool2d(2, stride=2)(x)\n",
    "            if i == 2:\n",
    "                x = torch.flatten(x, 1)\n",
    "            #### New lines for SVI ####\n",
    "            # This is added b/c o/w the gradient backprop w.r.t. new loss also gets to earlier layers (undesirable)\n",
    "            if self.layers_with_SVI[i] and self.on_training:\n",
    "                x_tmp = x.detach().clone().to(device)\n",
    "                x_tmp = conv(x_tmp)\n",
    "                if i < 2:\n",
    "                    x_tmp = self.bns[i](x_tmp)\n",
    "                self.layers_Xtilde.append(x_tmp)\n",
    "            #### End #####\n",
    "            x = conv(x)\n",
    "            if i < 2: \n",
    "                x = self.bns[i](x)\n",
    "            if i < len(self.convs) - 1:\n",
    "                x = nn.ReLU()(x)\n",
    "            #### New lines for SVI ####\n",
    "            if self.layers_with_SVI[i] and self.on_training:\n",
    "                x.retain_grad()  # To get the gradient with respect to output\n",
    "                self.layers_grad.append(x)\n",
    "            #### End #####\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGoC-5G_yxsU"
   },
   "source": [
    "### Utility function, including data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q92j2DdFsuq9"
   },
   "outputs": [],
   "source": [
    "def mem_report():\n",
    "    if device.type == 'cuda':\n",
    "        GPUs = GPUtil.getGPUs()\n",
    "        for i, gpu in enumerate(GPUs):\n",
    "            print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(\n",
    "                i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
    "    else:\n",
    "        print(\"CPU RAM Free: \"\n",
    "              + humanize.naturalsize(psutil.virtual_memory().available))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "cc8e20eb10034e6d83571dbb4de5e577",
      "5a46479b03fc498db0a7ec8c1769a23c",
      "ec822baea4d54ffe9ea220b36f6bc9f7",
      "248ae6c24ca2412ea996878f22dc86ee",
      "5d6514d10dbd4dfbbf5e39c7bb509a61",
      "07130a8da9604dd99e939667923267b3",
      "20c8b7f462bd4076bca5d00b54dd2f25",
      "a186b0b2fb2b4142be4b088de5e30215",
      "5d0a979aba644f4f83381b4fafecc257",
      "fd6d3e06c2bb44c889065787b9389dc6",
      "52c8df0956304649880aced8b60a415f"
     ]
    },
    "id": "ertPh34Q0yYA",
    "outputId": "c35c2428-0620-45d4-b28b-cd1da5e2cbe1"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "def subset_data(data,frac=1):\n",
    "    # Randomly subset a fraction of data from total data\n",
    "    np.random.seed(1103)\n",
    "    idx = np.random.choice(len(data),int(frac*len(data)),replace=False)\n",
    "    return torch.utils.data.Subset(data,idx)\n",
    "data_fixed = 'MNIST_batched' # 'MNIST_batched' or 'CIFAR10_batched'\n",
    "if 'MNIST' in data_fixed:\n",
    "    train_dataset0 = torchvision.datasets.MNIST(root = './data',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "    test_dataset0 = torchvision.datasets.MNIST(root = './data',\n",
    "                                              train = False,\n",
    "                                              transform = transforms.Compose([\n",
    "                                                      transforms.Resize((32,32)),\n",
    "                                                      transforms.ToTensor(),\n",
    "                                                      transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                              download=True)\n",
    "    num_classes = 10\n",
    "    in_channels = 1\n",
    "else:\n",
    "    # NOTE, CIFAR10 has color channels, so input size = 3*32*32 with FC net\n",
    "    train_dataset0 = torchvision.datasets.CIFAR10(root='/data', \n",
    "                                              train=True, \n",
    "                                              transform=transforms.ToTensor(),  \n",
    "                                              download=True)\n",
    "    test_dataset0 = torchvision.datasets.CIFAR10(root='/data', \n",
    "                                              train=False, \n",
    "                                              transform=transforms.ToTensor())\n",
    "    num_classes = 10 \n",
    "    in_channels = 3\n",
    "input_size = torch.prod(torch.tensor(train_dataset0[0][0].shape)).item()\n",
    "frac=0.1 if 'MNIST' in data_fixed else 0.2\n",
    "frac = 1\n",
    "train_dataset = subset_data(train_dataset0,frac=frac)\n",
    "test_dataset = subset_data(test_dataset0,frac=frac)\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "# Data loader (i.e., split to batches) see below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqAIlL3WUylQ"
   },
   "source": [
    "Start testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVI_ls = [True, True, False]  # If False, use ordinary SGD or Adam\n",
    "SVI_pause = [False, True, False]  # If True, only SVI to warm start\n",
    "# optim_ls = ['SGD', 'Adam']\n",
    "SVI_ls = [True]\n",
    "SVI_pause = [False]\n",
    "optim_ls = ['SGD']\n",
    "lr, hidden_channels = 0.001, 512\n",
    "FC_only = False  # Use all FC layers if True, o/w use LeNet\n",
    "dataname = data_fixed\n",
    "if FC_only == False:\n",
    "    dataname = data_fixed + 'LeNet_'\n",
    "num_runs = 3\n",
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "num_log_steps = len(train_dataset) // batch_size # Evaluate loss every X iteration/mini-batches\n",
    "if __name__ == \"__main__\":\n",
    "    Final_result = {}  # For quick check at the end of training\n",
    "    for use_SVI, pause_SVI in zip(SVI_ls, SVI_pause):\n",
    "        for optim_name in optim_ls:\n",
    "            result_dict = {'SVI-SGD': [],\n",
    "                           'SVI_warmstart-SGD': [],\n",
    "                           'SGD': [],\n",
    "                           'SVI-Adam': [],\n",
    "                           'SVI_warmstart-Adam': [],\n",
    "                           'Adam': []}\n",
    "            parser = argparse.ArgumentParser(\n",
    "                description=dataname)\n",
    "            parser.add_argument('--log_steps', type=int, default=num_log_steps)\n",
    "            parser.add_argument('--num_layers', type=int, default=4)\n",
    "            parser.add_argument('--dropout', type=float, default=0.25)\n",
    "            parser.add_argument('--lr', type=float, default=lr)\n",
    "            parser.add_argument('--momentum', type=float, default=0.95)\n",
    "            parser.add_argument(\n",
    "                '--epochs', type=int, default=num_epochs)  # Change to 100\n",
    "            parser.add_argument(\n",
    "                '--batch_size', type=int, default=batch_size)\n",
    "            parser.add_argument('--runs', type=int,\n",
    "                                default=num_runs)\n",
    "            parser.add_argument('--SVI', type=bool, default=use_SVI)\n",
    "            parser.add_argument('--FC', type=bool, default=FC_only)\n",
    "            parser.add_argument(\n",
    "                '--optimizer', type=str, default=optim_name)\n",
    "            args = parser.parse_args(args=[])\n",
    "            args.hidden_channels = hidden_channels\n",
    "            print(args)\n",
    "            # Get data loader from dataset\n",
    "            train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       shuffle=True)\n",
    "            test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      shuffle=False)\n",
    "            logger = Logger(args.runs, args)\n",
    "            results_over_runs = {}\n",
    "            for run in range(args.runs):\n",
    "                accu_at_run = []\n",
    "                # Initiatiate new model per run\n",
    "                torch.manual_seed(1103 + run)\n",
    "                args.SVI = use_SVI\n",
    "                if args.SVI:\n",
    "                    if args.FC:\n",
    "                        model = FCnet_SVI(input_size, args.hidden_channels,\n",
    "                                          num_classes, args.num_layers, args.dropout).to(device)\n",
    "                    else:\n",
    "                        model = LeNet5_SVI(in_channels, num_classes).to(device)\n",
    "                else:\n",
    "                    if args.FC:\n",
    "                        model = FCnet(input_size, args.hidden_channels,\n",
    "                                      num_classes, args.num_layers, args.dropout).to(device)\n",
    "                    else:\n",
    "                        model = LeNet5(in_channels, num_classes).to(device)\n",
    "                if args.optimizer == 'SGD':\n",
    "                    optimizer = torch.optim.SGD(\n",
    "                        model.parameters(), lr=args.lr, momentum=args.momentum, nesterov=True)\n",
    "                else:\n",
    "                    optimizer = torch.optim.Adam(\n",
    "                        model.parameters(), lr=args.lr)\n",
    "                for epoch in range(1, 1 + args.epochs):\n",
    "                    iter_per_epoch = int(len(train_dataset) / batch_size) + 1\n",
    "                    frac_stop = 0.1\n",
    "                    pause_threshold = int(\n",
    "                        frac_stop * iter_per_epoch * args.epochs)\n",
    "                    if device.type == 'cuda':\n",
    "                        # Useful to avoid GPU allocation excess\n",
    "                        torch.cuda.empty_cache()\n",
    "                    print(f\"LR is {optimizer.param_groups[0]['lr']}\")\n",
    "                    for i, (images, labels) in enumerate(train_loader):\n",
    "                        current_itr = ((epoch - 1) * iter_per_epoch) + i + 1\n",
    "                        if pause_SVI and current_itr > pause_threshold:\n",
    "                            args.SVI = False\n",
    "                            if current_itr <= pause_threshold + 1:\n",
    "                                # Reinitialize optimizer to avoid gradient issue in Adam\n",
    "                                sdict = model.state_dict()\n",
    "                                print(\n",
    "                                    '############ Pause SVI from now on ############')\n",
    "                                if args.FC:\n",
    "                                    model = FCnet(input_size, args.hidden_channels,\n",
    "                                                  num_classes, args.num_layers, args.dropout)\n",
    "                                else:\n",
    "                                    model = LeNet5(in_channels, num_classes)\n",
    "                                model.load_state_dict(sdict)\n",
    "                                model = model.to(device)\n",
    "                                if args.optimizer == 'SGD':\n",
    "                                    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, \n",
    "                                                                momentum=args.momentum, \n",
    "                                                                nesterov=True)\n",
    "                                else:\n",
    "                                    optimizer = torch.optim.Adam(\n",
    "                                        model.parameters(), lr=args.lr)\n",
    "                        optimizer.zero_grad()\n",
    "                        if args.FC:\n",
    "                            images = images.reshape(-1, input_size).to(device)\n",
    "                        images = images.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        loss = train_SVI(model, images, labels) if args.SVI else train(model, images, labels)\n",
    "                        optimizer.step()\n",
    "                        # Test at each iteration\n",
    "                        if (i+1) % args.log_steps == 0:\n",
    "                            print('Testing')\n",
    "                            if args.SVI:\n",
    "                                print(f'SVI-{args.optimizer} training at batch {i}, epoch {epoch}')\n",
    "                            else:\n",
    "                                print(f'{args.optimizer} training at batch {i}, epoch {epoch}')\n",
    "                            # Do so because training data has too many images, and evaluation thus takes too long.\n",
    "                            train_loader_sub = torch.utils.data.DataLoader(dataset=subset_data(train_dataset0,frac=0.1),\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       shuffle=True)\n",
    "                            result = test(model, train_loader_sub, test_loader)\n",
    "                            logger.add_result(run, result)\n",
    "                            train_acc, test_acc = result\n",
    "                            accu_at_run += [[train_acc, test_acc]]\n",
    "                            print(f'Run: {run + 1:02d}, '\n",
    "                                  f'Epoch: {epoch:02d}, '\n",
    "                                  f'Loss: {loss:.4f}, '\n",
    "                                  f'Train: {100 * train_acc:.2f}%,'\n",
    "                                  f'Test: {100 * test_acc:.2f}%')\n",
    "                # Save model after training over all epochs\n",
    "                results_over_runs[f'lr={args.lr}@Run{run+1}'] = accu_at_run\n",
    "                logger.print_statistics(run)\n",
    "                # Save results\n",
    "                if use_SVI:\n",
    "                    SVI_prefix = 'SVI_warmstart-' if pause_SVI else 'SVI-'\n",
    "                else:\n",
    "                    SVI_prefix = ''\n",
    "                key = f'{SVI_prefix}{optim_name}'\n",
    "                key_save = f'{SVI_prefix}{optim_name}-{args.num_layers}layers-{args.hidden_channels}nodes-{args.lr}LR'\n",
    "                key_save = dataname + key_save\n",
    "                # # Save it to file, but need not now because only one run.\n",
    "                # logger.pickle(key_save)\n",
    "                result_dict[key].append(results_over_runs)\n",
    "                # Final train and test accuracy\n",
    "                Final_result[key] = accu_at_run[-1]\n",
    "                with open(f\"{key_save}_loss_together_SVI_only.json\", \"w\") as outfile:\n",
    "                    json.dump(result_dict, outfile)\n",
    "            logger.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is if we always use SVI, not just warm start\n",
    "# All runs:\n",
    "# Highest Train: 97.91 ± 0.18\n",
    "# Highest Test: 97.80 ± 0.13\n",
    "#   Final Train: 97.91 ± 0.18\n",
    "#    Final Test: 97.80 ± 0.13\n",
    "logger.print_statistics()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8Ekr8mjqiXld",
    "DJp7BXpnP-5Q",
    "e-jvjKsmxOZO"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07130a8da9604dd99e939667923267b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20c8b7f462bd4076bca5d00b54dd2f25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "248ae6c24ca2412ea996878f22dc86ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd6d3e06c2bb44c889065787b9389dc6",
      "placeholder": "​",
      "style": "IPY_MODEL_52c8df0956304649880aced8b60a415f",
      "value": " 170498071/170498071 [00:03&lt;00:00, 54226204.17it/s]"
     }
    },
    "52c8df0956304649880aced8b60a415f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a46479b03fc498db0a7ec8c1769a23c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07130a8da9604dd99e939667923267b3",
      "placeholder": "​",
      "style": "IPY_MODEL_20c8b7f462bd4076bca5d00b54dd2f25",
      "value": "100%"
     }
    },
    "5d0a979aba644f4f83381b4fafecc257": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d6514d10dbd4dfbbf5e39c7bb509a61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a186b0b2fb2b4142be4b088de5e30215": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc8e20eb10034e6d83571dbb4de5e577": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a46479b03fc498db0a7ec8c1769a23c",
       "IPY_MODEL_ec822baea4d54ffe9ea220b36f6bc9f7",
       "IPY_MODEL_248ae6c24ca2412ea996878f22dc86ee"
      ],
      "layout": "IPY_MODEL_5d6514d10dbd4dfbbf5e39c7bb509a61"
     }
    },
    "ec822baea4d54ffe9ea220b36f6bc9f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a186b0b2fb2b4142be4b088de5e30215",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d0a979aba644f4f83381b4fafecc257",
      "value": 170498071
     }
    },
    "fd6d3e06c2bb44c889065787b9389dc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
