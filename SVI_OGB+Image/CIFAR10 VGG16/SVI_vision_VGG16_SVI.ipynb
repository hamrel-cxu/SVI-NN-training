{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRq0syKC5lvV"
   },
   "source": [
    "##  Let us begin\n",
    "\n",
    "Can add descriptions later for users to load interactively. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sdRF5Nr3zCQy",
    "outputId": "c3efc159-9669-480d-d9c4-10d05943550d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100 80GB PCIe (UUID: GPU-6751f8a9-b574-c4b1-f722-e7d8225018ed)\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0LfvspJpN1x",
    "outputId": "9ee675d5-b1b1-4cb8-86f4-62ca4f06758c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting GPUtil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: GPUtil\n",
      "  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7394 sha256=d9c61cbed87d5d7ff6a6e507014564df76dfe1941e651dc7fb90ffa709fd249b\n",
      "  Stored in directory: /storage/home/hcoda1/9/cxu310/.cache/pip/wheels/2b/b5/24/fbb56595c286984f7315ee31821d6121e1b9828436021a88b3\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: GPUtil\n",
      "Successfully installed GPUtil-1.4.0\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# !pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "q6oQVsXSLJW-"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import sys\n",
    "import importlib as ipb\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import GPUtil\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VFMP5RgRYnhA"
   },
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "      \n",
    "    def pickle(self, key_save):\n",
    "        f = open(key_save, 'wb')\n",
    "        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "\n",
    "    def unpickle(self, key_save):\n",
    "        with open(key_save, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 2\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 0].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Test: {result[:, 1].max():.2f}')\n",
    "            # Same as highest train, as we have no validation data\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}') \n",
    "            print(f'   Final Test: {result[argmax, 1]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train = r[:, 0].max().item()\n",
    "                test = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 0].argmax(), 0].item()\n",
    "                test2 = r[r[:, 0].argmax(), 1].item()\n",
    "                best_results.append((train, test, train2, test2))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Test: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')\n",
    "\n",
    "\n",
    "def test(model, train_loader, test_loader):\n",
    "    model.eval()\n",
    "    loader = {0: train_loader, 1:test_loader}\n",
    "    accuracies = []\n",
    "    with torch.no_grad():\n",
    "        for data_loader in loader.values():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, (images, labels) in enumerate(data_loader):\n",
    "                if args.FC:\n",
    "                    images = images.reshape(-1, input_size).to(device)\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            accuracies.append(correct/total)\n",
    "    return accuracies # train_accu & test accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "se13vA7gjTeh"
   },
   "source": [
    "### Training function (include SVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Zvp-50gniy1w"
   },
   "outputs": [],
   "source": [
    "def train(model, images, labels):\n",
    "    model.train()\n",
    "    out = model(images)\n",
    "    cost = nn.CrossEntropyLoss()\n",
    "    loss = cost(out, labels)\n",
    "    loss.backward()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train_SVI(model, images, labels):\n",
    "    #### New lines for SVI ####\n",
    "    # NOTE: lines below are necessary, as o/w model.layers_x grow in size as epoches increases\n",
    "    model.layers_Xtilde = []\n",
    "    model.layers_grad = []\n",
    "    model.on_training = True\n",
    "    #### End #####\n",
    "    model.train()\n",
    "    out = model(images)\n",
    "    cost = nn.CrossEntropyLoss()\n",
    "    loss = cost(out, labels)\n",
    "    #### New lines for SVI ####\n",
    "    model.turn_on_off_grad(on = False) \n",
    "    # print('#### Grad of model params before SVI')\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.grad is not None:\n",
    "    #         print(name, param.grad.norm())\n",
    "    #### End #####\n",
    "    loss.backward()\n",
    "    #### New lines for SVI ####\n",
    "    loss_tilde = 0\n",
    "    for Xlplus1, Xlplus1grad in zip(model.layers_Xtilde, model.layers_grad):\n",
    "        Xlplus1grad = Xlplus1grad.grad.detach().to(device)\n",
    "        loss_tilde += (Xlplus1*Xlplus1grad).sum()\n",
    "    model.turn_on_off_grad(on = True)\n",
    "    loss_tilde.backward()  # To get update direction by MVI for all layers at once\n",
    "    # print('#### Grad of model params after SVI')\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.grad is not None:\n",
    "    #         print(name, param.grad.norm())\n",
    "    # raise Exception('Stop here')\n",
    "    model.on_training = False  # To avoid additional .retain_grad()\n",
    "    #### End #####\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self,num_classes = 10):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        for layer in self.features:\n",
    "            x = layer(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class VGG16_SVI(nn.Module):\n",
    "    def __init__(self,num_classes = 10):\n",
    "        super(VGG16_SVI, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        #### New lines for SVI ####\n",
    "        self.layers_Xtilde = []\n",
    "        self.layers_grad = []\n",
    "        self.on_training = True\n",
    "        #### End #####\n",
    "\n",
    "    #### New lines for SVI ####\n",
    "    # Avoid gradient accumulation\n",
    "    def turn_on_off_grad(self, on = True):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = on\n",
    "\n",
    "    def append_to_layers_Xtilde(self, conv, x):\n",
    "        if self.on_training:\n",
    "            x_tmp = x.detach().clone().to(device)\n",
    "            if len(conv) > 1:\n",
    "                for conv_ in conv:\n",
    "                    x_tmp = conv_(x_tmp)\n",
    "                    if isinstance(conv_, nn.AdaptiveAvgPool2d):\n",
    "                        x_tmp = torch.flatten(x_tmp, 1)\n",
    "            else:\n",
    "                x_tmp = conv[0](x_tmp)\n",
    "            self.layers_Xtilde.append(x_tmp)\n",
    "    \n",
    "    def append_to_layers_grad(self, x):\n",
    "        if self.on_training:\n",
    "            x.retain_grad()\n",
    "            self.layers_grad.append(x)\n",
    "    #### End #####\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Manually breaks down SVI into pre-activation and post-activation\n",
    "        # (0) Conv2d\n",
    "        self.append_to_layers_Xtilde([self.features[0]], x)\n",
    "        x = self.features[0](x)\n",
    "        x = self.features[1](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (2) Conv2d\n",
    "        self.append_to_layers_Xtilde([self.features[2]], x)\n",
    "        x = self.features[2](x)\n",
    "        x = self.features[3](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (4) MaxPool2d + (5) Conv2d\n",
    "        self.append_to_layers_Xtilde([self.features[4], self.features[5]], x)\n",
    "        x = self.features[4](x)\n",
    "        x = self.features[5](x)\n",
    "        x = self.features[6](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (7) Conv2d\n",
    "        self.append_to_layers_Xtilde([self.features[7]], x)\n",
    "        x = self.features[7](x)\n",
    "        x = self.features[8](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (9) MaxPool2d + (10) Conv2d\n",
    "        self.append_to_layers_Xtilde([self.features[9], self.features[10]], x)\n",
    "        x = self.features[9](x)\n",
    "        x = self.features[10](x)\n",
    "        x = self.features[11](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (12) Conv2d\n",
    "        self.append_to_layers_Xtilde([self.features[12]], x)\n",
    "        x = self.features[12](x)\n",
    "        x = self.features[13](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (14) Conv2d\n",
    "        self.append_to_layers_Xtilde([self.features[14]], x)\n",
    "        x = self.features[14](x)\n",
    "        x = self.features[15](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (16) MaxPool2d + (17) Conv2d\n",
    "        self.append_to_layers_Xtilde([self.features[16],self.features[17]], x)\n",
    "        x = self.features[16](x)\n",
    "        x = self.features[17](x)\n",
    "        x = self.features[18](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (19) Conv2d\n",
    "        self.append_to_layers_Xtilde([self.features[19]], x)\n",
    "        x = self.features[19](x)\n",
    "        x = self.features[20](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (21) Conv2d\n",
    "        self.append_to_layers_Xtilde([self.features[21]], x)\n",
    "        x = self.features[21](x)\n",
    "        x = self.features[22](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (23) MaxPool2d + (24) Conv2d\n",
    "        self.append_to_layers_Xtilde([self.features[23], self.features[24]], x)\n",
    "        x = self.features[23](x)\n",
    "        x = self.features[24](x)\n",
    "        x = self.features[25](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (26) Conv2d\n",
    "        self.append_to_layers_Xtilde([self.features[26]], x)\n",
    "        x = self.features[26](x)\n",
    "        x = self.features[27](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (28) Conv2d\n",
    "        self.append_to_layers_Xtilde([self.features[28]], x)\n",
    "        x = self.features[28](x)\n",
    "        x = self.features[29](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (30) MaxPool2d + avgpool + flatten + (0) Linear\n",
    "        self.append_to_layers_Xtilde([self.features[30],self.avgpool, self.classifier[0]], x)\n",
    "        x = self.features[30](x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier[0](x)\n",
    "        x = self.classifier[1](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (2) Dropout + (3) Linear\n",
    "        self.append_to_layers_Xtilde([self.classifier[2], self.classifier[3]], x)\n",
    "        x = self.classifier[2](x)\n",
    "        x = self.classifier[3](x)\n",
    "        x = self.classifier[4](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        # (5) Dropout + (6) Linear\n",
    "        self.append_to_layers_Xtilde([self.classifier[5], self.classifier[6]], x)\n",
    "        x = self.classifier[5](x)\n",
    "        x = self.classifier[6](x)\n",
    "        self.append_to_layers_grad(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/pace-apps/manual/packages/pytorch/1.12.0/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /storage/home/hcoda1/9/cxu310/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530f010c2c0542f0ab0ed2d409616f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "model_pretrained = models.vgg16(pretrained = True)\n",
    "\n",
    "def vgg_16_load(SVI = False, load_pretrain = False):\n",
    "    if load_pretrain:\n",
    "        num_c = 1000\n",
    "        model = VGG16_SVI(num_c) if SVI else VGG16(num_c)\n",
    "        model.load_state_dict(model_pretrained.state_dict())\n",
    "        model.classifier[6] = nn.Linear(4096, 10)\n",
    "    else:\n",
    "        num_c = 10\n",
    "        model = VGG16_SVI(num_c) if SVI else VGG16(num_c)\n",
    "    return model.to(device) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod1 = vgg_16_load(SVI = True, load_pretrain = True)\n",
    "x = torch.randn(10, 3, 32, 32).to(device)\n",
    "out = mod1(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 16\n"
     ]
    }
   ],
   "source": [
    "print(len(mod1.layers_Xtilde), len(mod1.layers_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGoC-5G_yxsU"
   },
   "source": [
    "### Utility function, including data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "q92j2DdFsuq9"
   },
   "outputs": [],
   "source": [
    "def mem_report():\n",
    "    if device.type == 'cuda':\n",
    "        GPUs = GPUtil.getGPUs()\n",
    "        for i, gpu in enumerate(GPUs):\n",
    "            print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(\n",
    "                i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
    "    else:\n",
    "        print(\"CPU RAM Free: \"\n",
    "              + humanize.naturalsize(psutil.virtual_memory().available))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "cc8e20eb10034e6d83571dbb4de5e577",
      "5a46479b03fc498db0a7ec8c1769a23c",
      "ec822baea4d54ffe9ea220b36f6bc9f7",
      "248ae6c24ca2412ea996878f22dc86ee",
      "5d6514d10dbd4dfbbf5e39c7bb509a61",
      "07130a8da9604dd99e939667923267b3",
      "20c8b7f462bd4076bca5d00b54dd2f25",
      "a186b0b2fb2b4142be4b088de5e30215",
      "5d0a979aba644f4f83381b4fafecc257",
      "fd6d3e06c2bb44c889065787b9389dc6",
      "52c8df0956304649880aced8b60a415f"
     ]
    },
    "id": "ertPh34Q0yYA",
    "outputId": "c35c2428-0620-45d4-b28b-cd1da5e2cbe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2212d2b75d54664aba4da2daaca0b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "def subset_data(data,frac=1):\n",
    "    # Randomly subset a fraction of data from total data\n",
    "    np.random.seed(1103)\n",
    "    idx = np.random.choice(len(data),int(frac*len(data)),replace=False)\n",
    "    return torch.utils.data.Subset(data,idx)\n",
    "data_fixed = 'CIFAR10_batched' # 'MNIST_batched' or 'CIFAR10_batched'\n",
    "if 'MNIST' in data_fixed:\n",
    "    train_dataset0 = torchvision.datasets.MNIST(root = './data',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "    test_dataset0 = torchvision.datasets.MNIST(root = './data',\n",
    "                                              train = False,\n",
    "                                              transform = transforms.Compose([\n",
    "                                                      transforms.Resize((32,32)),\n",
    "                                                      transforms.ToTensor(),\n",
    "                                                      transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                              download=True)\n",
    "    num_classes = 10\n",
    "    in_channels = 1\n",
    "else:\n",
    "    # NOTE, CIFAR10 has color channels, so input size = 3*32*32 with FC net\n",
    "    train_dataset0 = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                              train=True, \n",
    "                                              transform=transforms.ToTensor(),  \n",
    "                                              download=True)\n",
    "    test_dataset0 = torchvision.datasets.CIFAR10(root='./data', \n",
    "                                              train=False, \n",
    "                                              transform=transforms.ToTensor())\n",
    "    num_classes = 10 \n",
    "    in_channels = 3\n",
    "input_size = torch.prod(torch.tensor(train_dataset0[0][0].shape)).item()\n",
    "frac=0.1 if 'MNIST' in data_fixed else 0.2\n",
    "frac = 1\n",
    "train_dataset = subset_data(train_dataset0,frac=frac)\n",
    "test_dataset = subset_data(test_dataset0,frac=frac)\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "# Data loader (i.e., split to batches) see below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqAIlL3WUylQ"
   },
   "source": [
    "Start testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(log_steps=390, num_layers=4, dropout=0.25, lr=0.001, momentum=0.95, epochs=20, batch_size=128, runs=3, SVI=True, FC=False, optimizer='SGD', hidden_channels=512)\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 389, epoch 1\n",
      "Run: 01, Epoch: 01, Loss: 0.4670, Train: 83.72%,Test: 80.80%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 389, epoch 2\n",
      "Run: 01, Epoch: 02, Loss: 0.3883, Train: 87.22%,Test: 83.28%\n",
      "LR is 0.001\n",
      "############ Pause SVI from now on ############\n",
      "Testing\n",
      "SGD training at batch 389, epoch 3\n",
      "Run: 01, Epoch: 03, Loss: 0.2588, Train: 91.42%,Test: 85.15%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 4\n",
      "Run: 01, Epoch: 04, Loss: 0.3008, Train: 94.00%,Test: 85.93%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 5\n",
      "Run: 01, Epoch: 05, Loss: 0.1506, Train: 96.88%,Test: 86.47%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 6\n",
      "Run: 01, Epoch: 06, Loss: 0.1101, Train: 98.26%,Test: 86.78%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 7\n",
      "Run: 01, Epoch: 07, Loss: 0.1497, Train: 98.46%,Test: 86.66%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 8\n",
      "Run: 01, Epoch: 08, Loss: 0.0810, Train: 99.36%,Test: 86.82%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 9\n",
      "Run: 01, Epoch: 09, Loss: 0.0441, Train: 99.46%,Test: 87.01%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 10\n",
      "Run: 01, Epoch: 10, Loss: 0.0184, Train: 98.86%,Test: 86.24%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 11\n",
      "Run: 01, Epoch: 11, Loss: 0.0079, Train: 99.04%,Test: 86.98%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 12\n",
      "Run: 01, Epoch: 12, Loss: 0.0372, Train: 99.16%,Test: 87.03%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 13\n",
      "Run: 01, Epoch: 13, Loss: 0.0418, Train: 99.42%,Test: 86.67%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 14\n",
      "Run: 01, Epoch: 14, Loss: 0.0504, Train: 98.94%,Test: 86.10%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 15\n",
      "Run: 01, Epoch: 15, Loss: 0.0056, Train: 99.62%,Test: 87.34%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 16\n",
      "Run: 01, Epoch: 16, Loss: 0.0758, Train: 99.40%,Test: 86.76%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 17\n",
      "Run: 01, Epoch: 17, Loss: 0.0207, Train: 99.76%,Test: 87.45%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 18\n",
      "Run: 01, Epoch: 18, Loss: 0.0011, Train: 99.82%,Test: 87.40%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 19\n",
      "Run: 01, Epoch: 19, Loss: 0.0296, Train: 99.78%,Test: 87.18%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 20\n",
      "Run: 01, Epoch: 20, Loss: 0.0025, Train: 99.32%,Test: 86.83%\n",
      "Run 01:\n",
      "Highest Train: 99.82\n",
      "Highest Test: 87.45\n",
      "  Final Train: 99.82\n",
      "   Final Test: 87.40\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 389, epoch 1\n",
      "Run: 02, Epoch: 01, Loss: 0.7679, Train: 83.02%,Test: 80.22%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 389, epoch 2\n",
      "Run: 02, Epoch: 02, Loss: 0.5958, Train: 87.70%,Test: 83.79%\n",
      "LR is 0.001\n",
      "############ Pause SVI from now on ############\n",
      "Testing\n",
      "SGD training at batch 389, epoch 3\n",
      "Run: 02, Epoch: 03, Loss: 0.3462, Train: 91.04%,Test: 84.93%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 4\n",
      "Run: 02, Epoch: 04, Loss: 0.3311, Train: 94.58%,Test: 85.76%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 5\n",
      "Run: 02, Epoch: 05, Loss: 0.1622, Train: 96.80%,Test: 86.59%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 6\n",
      "Run: 02, Epoch: 06, Loss: 0.1393, Train: 98.10%,Test: 86.95%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 7\n",
      "Run: 02, Epoch: 07, Loss: 0.0505, Train: 98.14%,Test: 86.95%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 8\n",
      "Run: 02, Epoch: 08, Loss: 0.0438, Train: 99.00%,Test: 86.48%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 9\n",
      "Run: 02, Epoch: 09, Loss: 0.0537, Train: 99.48%,Test: 87.12%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 10\n",
      "Run: 02, Epoch: 10, Loss: 0.0713, Train: 99.36%,Test: 86.81%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 11\n",
      "Run: 02, Epoch: 11, Loss: 0.0063, Train: 99.20%,Test: 86.79%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 12\n",
      "Run: 02, Epoch: 12, Loss: 0.0229, Train: 99.32%,Test: 86.49%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 13\n",
      "Run: 02, Epoch: 13, Loss: 0.0436, Train: 99.62%,Test: 87.23%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 14\n",
      "Run: 02, Epoch: 14, Loss: 0.0112, Train: 99.48%,Test: 86.69%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 15\n",
      "Run: 02, Epoch: 15, Loss: 0.0188, Train: 99.62%,Test: 87.10%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 16\n",
      "Run: 02, Epoch: 16, Loss: 0.0092, Train: 99.62%,Test: 87.38%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 17\n",
      "Run: 02, Epoch: 17, Loss: 0.0650, Train: 99.78%,Test: 87.39%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 18\n",
      "Run: 02, Epoch: 18, Loss: 0.0052, Train: 99.54%,Test: 87.12%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 19\n",
      "Run: 02, Epoch: 19, Loss: 0.0028, Train: 99.68%,Test: 87.63%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 20\n",
      "Run: 02, Epoch: 20, Loss: 0.0066, Train: 99.82%,Test: 87.80%\n",
      "Run 02:\n",
      "Highest Train: 99.82\n",
      "Highest Test: 87.80\n",
      "  Final Train: 99.82\n",
      "   Final Test: 87.80\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 389, epoch 1\n",
      "Run: 03, Epoch: 01, Loss: 0.4866, Train: 83.16%,Test: 81.07%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SVI-SGD training at batch 389, epoch 2\n",
      "Run: 03, Epoch: 02, Loss: 0.5035, Train: 85.98%,Test: 82.85%\n",
      "LR is 0.001\n",
      "############ Pause SVI from now on ############\n",
      "Testing\n",
      "SGD training at batch 389, epoch 3\n",
      "Run: 03, Epoch: 03, Loss: 0.2277, Train: 91.82%,Test: 85.34%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 4\n",
      "Run: 03, Epoch: 04, Loss: 0.2055, Train: 94.24%,Test: 85.91%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 5\n",
      "Run: 03, Epoch: 05, Loss: 0.2139, Train: 96.38%,Test: 86.33%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 6\n",
      "Run: 03, Epoch: 06, Loss: 0.2085, Train: 97.82%,Test: 86.67%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 7\n",
      "Run: 03, Epoch: 07, Loss: 0.0819, Train: 98.46%,Test: 86.50%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 8\n",
      "Run: 03, Epoch: 08, Loss: 0.0216, Train: 99.38%,Test: 86.84%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 9\n",
      "Run: 03, Epoch: 09, Loss: 0.0615, Train: 99.12%,Test: 86.63%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 10\n",
      "Run: 03, Epoch: 10, Loss: 0.0261, Train: 99.32%,Test: 86.56%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 11\n",
      "Run: 03, Epoch: 11, Loss: 0.0147, Train: 99.68%,Test: 87.33%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 12\n",
      "Run: 03, Epoch: 12, Loss: 0.0081, Train: 99.18%,Test: 86.93%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 13\n",
      "Run: 03, Epoch: 13, Loss: 0.0268, Train: 99.24%,Test: 86.48%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 14\n",
      "Run: 03, Epoch: 14, Loss: 0.0023, Train: 99.80%,Test: 87.16%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 15\n",
      "Run: 03, Epoch: 15, Loss: 0.0053, Train: 99.50%,Test: 87.01%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 16\n",
      "Run: 03, Epoch: 16, Loss: 0.0141, Train: 99.80%,Test: 87.00%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 17\n",
      "Run: 03, Epoch: 17, Loss: 0.0095, Train: 99.76%,Test: 87.43%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 18\n",
      "Run: 03, Epoch: 18, Loss: 0.0073, Train: 99.86%,Test: 87.40%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 19\n",
      "Run: 03, Epoch: 19, Loss: 0.0021, Train: 99.82%,Test: 87.21%\n",
      "LR is 0.001\n",
      "Testing\n",
      "SGD training at batch 389, epoch 20\n",
      "Run: 03, Epoch: 20, Loss: 0.0179, Train: 99.76%,Test: 87.23%\n",
      "Run 03:\n",
      "Highest Train: 99.86\n",
      "Highest Test: 87.43\n",
      "  Final Train: 99.86\n",
      "   Final Test: 87.40\n",
      "All runs:\n",
      "Highest Train: 99.83 ± 0.02\n",
      "Highest Test: 87.56 ± 0.21\n",
      "  Final Train: 99.83 ± 0.02\n",
      "   Final Test: 87.53 ± 0.23\n"
     ]
    }
   ],
   "source": [
    "SVI_ls = [True, True, False]  # If False, use ordinary SGD or Adam\n",
    "SVI_pause = [False, True, False]  # If True, only SVI to warm start\n",
    "SVI_ls = [True, False]\n",
    "SVI_pause = [True, False]\n",
    "SVI_ls = [True]\n",
    "SVI_pause = [True]\n",
    "optim_ls = ['SGD']\n",
    "load_pretrain = True # Load pretrained VGG16 to transfer learning\n",
    "lr, hidden_channels = 0.001, 512\n",
    "FC_only = False  # Use all FC layers if True, o/w use LeNet\n",
    "dataname = data_fixed\n",
    "if FC_only == False:\n",
    "    dataname = data_fixed + 'VGG16_'\n",
    "num_runs = 3\n",
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "num_log_steps = len(train_dataset) // batch_size # Evaluate loss every X iteration/mini-batches\n",
    "if __name__ == \"__main__\":\n",
    "    Final_result = {}  # For quick check at the end of training\n",
    "    for use_SVI, pause_SVI in zip(SVI_ls, SVI_pause):\n",
    "        for optim_name in optim_ls:\n",
    "            result_dict = {'SVI-SGD': [],\n",
    "                           'SVI_warmstart-SGD': [],\n",
    "                           'SGD': [],\n",
    "                           'SVI-Adam': [],\n",
    "                           'SVI_warmstart-Adam': [],\n",
    "                           'Adam': []}\n",
    "            parser = argparse.ArgumentParser(\n",
    "                description=dataname)\n",
    "            parser.add_argument('--log_steps', type=int, default=num_log_steps)\n",
    "            parser.add_argument('--num_layers', type=int, default=4)\n",
    "            parser.add_argument('--dropout', type=float, default=0.25)\n",
    "            parser.add_argument('--lr', type=float, default=lr)\n",
    "            parser.add_argument('--momentum', type=float, default=0.95)\n",
    "            parser.add_argument(\n",
    "                '--epochs', type=int, default=num_epochs)  # Change to 100\n",
    "            parser.add_argument(\n",
    "                '--batch_size', type=int, default=batch_size)\n",
    "            parser.add_argument('--runs', type=int,\n",
    "                                default=num_runs)\n",
    "            parser.add_argument('--SVI', type=bool, default=use_SVI)\n",
    "            parser.add_argument('--FC', type=bool, default=FC_only)\n",
    "            parser.add_argument(\n",
    "                '--optimizer', type=str, default=optim_name)\n",
    "            args = parser.parse_args(args=[])\n",
    "            args.hidden_channels = hidden_channels\n",
    "            print(args)\n",
    "            # Get data loader from dataset\n",
    "            train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       shuffle=True)\n",
    "            test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      shuffle=False)\n",
    "            logger = Logger(args.runs, args)\n",
    "            results_over_runs = {}\n",
    "            for run in range(args.runs):\n",
    "                accu_at_run = []\n",
    "                # Initiatiate new model per run\n",
    "                torch.manual_seed(1103 + run)\n",
    "                args.SVI = use_SVI\n",
    "                model = vgg_16_load(args.SVI, load_pretrain = load_pretrain)\n",
    "                if args.optimizer == 'SGD':\n",
    "                    optimizer = torch.optim.SGD(\n",
    "                        model.parameters(), lr=args.lr, momentum=args.momentum, nesterov=True)\n",
    "                else:\n",
    "                    optimizer = torch.optim.Adam(\n",
    "                        model.parameters(), lr=args.lr)\n",
    "                for epoch in range(1, 1 + args.epochs):\n",
    "                    iter_per_epoch = int(len(train_dataset) / batch_size) + 1\n",
    "                    frac_stop = 0.1\n",
    "                    if num_epochs > 20:\n",
    "                        frac_stop = 0.05\n",
    "                    pause_threshold = int(\n",
    "                        frac_stop * iter_per_epoch * args.epochs)\n",
    "                    if device.type == 'cuda':\n",
    "                        # Useful to avoid GPU allocation excess\n",
    "                        torch.cuda.empty_cache()\n",
    "                    print(f\"LR is {optimizer.param_groups[0]['lr']}\")\n",
    "                    for i, (images, labels) in enumerate(train_loader):\n",
    "                        current_itr = ((epoch - 1) * iter_per_epoch) + i + 1\n",
    "                        if pause_SVI and current_itr > pause_threshold:\n",
    "                            args.SVI = False\n",
    "                            if current_itr <= pause_threshold + 1:\n",
    "                                # Reinitialize optimizer to avoid gradient issue in Adam\n",
    "                                sdict = model.state_dict()\n",
    "                                print(\n",
    "                                    '############ Pause SVI from now on ############')\n",
    "                                model = vgg_16_load(args.SVI, load_pretrain = load_pretrain)\n",
    "                                model.load_state_dict(sdict)\n",
    "                                model = model.to(device)\n",
    "                                if args.optimizer == 'SGD':\n",
    "                                    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, \n",
    "                                                                momentum=args.momentum, \n",
    "                                                                nesterov=True)\n",
    "                                else:\n",
    "                                    optimizer = torch.optim.Adam(\n",
    "                                        model.parameters(), lr=args.lr)\n",
    "                        optimizer.zero_grad()\n",
    "                        if args.FC:\n",
    "                            images = images.reshape(-1, input_size).to(device)\n",
    "                        images = images.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        loss = train_SVI(model, images, labels) if args.SVI else train(model, images, labels)\n",
    "                        optimizer.step()\n",
    "                        # Test at each iteration\n",
    "                        if (i+1) % args.log_steps == 0:\n",
    "                            print('Testing')\n",
    "                            if args.SVI:\n",
    "                                print(f'SVI-{args.optimizer} training at batch {i}, epoch {epoch}')\n",
    "                            else:\n",
    "                                print(f'{args.optimizer} training at batch {i}, epoch {epoch}')\n",
    "                            # Do so because training data has too many images, and evaluation thus takes too long.\n",
    "                            train_loader_sub = torch.utils.data.DataLoader(dataset=subset_data(train_dataset0,frac=0.1),\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       shuffle=True)\n",
    "                            result = test(model, train_loader_sub, test_loader)\n",
    "                            logger.add_result(run, result)\n",
    "                            train_acc, test_acc = result\n",
    "                            accu_at_run += [[train_acc, test_acc]]\n",
    "                            print(f'Run: {run + 1:02d}, '\n",
    "                                  f'Epoch: {epoch:02d}, '\n",
    "                                  f'Loss: {loss:.4f}, '\n",
    "                                  f'Train: {100 * train_acc:.2f}%,'\n",
    "                                  f'Test: {100 * test_acc:.2f}%')\n",
    "                # Save model after training over all epochs\n",
    "                results_over_runs[f'lr={args.lr}@Run{run+1}'] = accu_at_run\n",
    "                logger.print_statistics(run)\n",
    "                # Save results\n",
    "                if use_SVI:\n",
    "                    SVI_prefix = 'SVI_warmstart-' if pause_SVI else 'SVI-'\n",
    "                else:\n",
    "                    SVI_prefix = ''\n",
    "                key = f'{SVI_prefix}{optim_name}'\n",
    "                key_save = f'{SVI_prefix}{optim_name}-{args.num_layers}layers-{args.hidden_channels}nodes-{args.lr}LR'\n",
    "                key_save = dataname + key_save\n",
    "                # # Save it to file, but need not now because only one run.\n",
    "                # logger.pickle(key_save)\n",
    "                result_dict[key].append(results_over_runs)\n",
    "                # Final train and test accuracy\n",
    "                Final_result[key] = accu_at_run[-1]\n",
    "                epoch_suff = '_more_epoch' if num_epochs > 20 else ''\n",
    "                with open(f\"{key_save}_loss_together_SVI_only.json\", \"w\") as outfile:\n",
    "                    json.dump(result_dict, outfile)\n",
    "            logger.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "8Ekr8mjqiXld",
    "DJp7BXpnP-5Q",
    "e-jvjKsmxOZO"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07130a8da9604dd99e939667923267b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20c8b7f462bd4076bca5d00b54dd2f25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "248ae6c24ca2412ea996878f22dc86ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd6d3e06c2bb44c889065787b9389dc6",
      "placeholder": "​",
      "style": "IPY_MODEL_52c8df0956304649880aced8b60a415f",
      "value": " 170498071/170498071 [00:03&lt;00:00, 54226204.17it/s]"
     }
    },
    "52c8df0956304649880aced8b60a415f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a46479b03fc498db0a7ec8c1769a23c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07130a8da9604dd99e939667923267b3",
      "placeholder": "​",
      "style": "IPY_MODEL_20c8b7f462bd4076bca5d00b54dd2f25",
      "value": "100%"
     }
    },
    "5d0a979aba644f4f83381b4fafecc257": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d6514d10dbd4dfbbf5e39c7bb509a61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a186b0b2fb2b4142be4b088de5e30215": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc8e20eb10034e6d83571dbb4de5e577": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a46479b03fc498db0a7ec8c1769a23c",
       "IPY_MODEL_ec822baea4d54ffe9ea220b36f6bc9f7",
       "IPY_MODEL_248ae6c24ca2412ea996878f22dc86ee"
      ],
      "layout": "IPY_MODEL_5d6514d10dbd4dfbbf5e39c7bb509a61"
     }
    },
    "ec822baea4d54ffe9ea220b36f6bc9f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a186b0b2fb2b4142be4b088de5e30215",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d0a979aba644f4f83381b4fafecc257",
      "value": 170498071
     }
    },
    "fd6d3e06c2bb44c889065787b9389dc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
