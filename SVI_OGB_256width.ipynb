{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OGB arxiv paper node classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torch torchvision torchaudio --y\n",
    "!pip install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
    "!pip install torch_geometric\n",
    "!pip install ogb\n",
    "!pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import json\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "import GPUtil\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "      \n",
    "    def pickle(self, key_save):\n",
    "        f = open(key_save, 'wb')\n",
    "        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "\n",
    "    def unpickle(self, key_save):\n",
    "        with open(key_save, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 3\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'Highest Test: {result[:, 2].max():.2f}')\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'  Final Valid: {result[argmax, 1]:.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                test = r[:, 2].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test2 = r[r[:, 1].argmax(), 2].item()\n",
    "                best_results.append((train, valid, test, train2, test2))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'Highest Test: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'  Final Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 4]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')\n",
    "\n",
    "\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.adj_t)\n",
    "        y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "        train_acc = evaluator.eval({\n",
    "            'y_true': data.y[split_idx['train']],\n",
    "            'y_pred': y_pred[split_idx['train']],\n",
    "        })['acc']\n",
    "        valid_acc = evaluator.eval({\n",
    "            'y_true': data.y[split_idx['valid']],\n",
    "            'y_pred': y_pred[split_idx['valid']],\n",
    "        })['acc']\n",
    "        test_acc = evaluator.eval({\n",
    "            'y_true': data.y[split_idx['test']],\n",
    "            'y_pred': y_pred[split_idx['test']],\n",
    "        })['acc']\n",
    "    return train_acc, valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_idx(data, indices):\n",
    "    full_mat = data.adj_t.to_scipy().tocsr()\n",
    "    coo = full_mat[indices][:, indices].tocoo() # This is wrong, because it omits \"papers\" in the future\n",
    "    values = coo.data\n",
    "    sub_indices = np.vstack((coo.row, coo.col))\n",
    "    i = torch.LongTensor(sub_indices)\n",
    "    v = torch.FloatTensor(values)\n",
    "    shape = coo.shape\n",
    "    sub_idx = torch.sparse.FloatTensor(i, v, torch.Size(shape)).coalesce().indices().clone()\n",
    "    sub_x = data.x[indices].clone()\n",
    "    sub_y = data.y[indices].clone()\n",
    "    return Data(x=sub_x, y=sub_y, edge_index=sub_idx).to(device)\n",
    "\n",
    "# Combine and resplit split_idx\n",
    "def shuffle_split_idx(split_idx):\n",
    "    train_idx = split_idx['train']\n",
    "    val_idx = split_idx['valid']\n",
    "    test_idx = split_idx['test']\n",
    "    full_idx = torch.cat([train_idx, val_idx, test_idx])\n",
    "    train_frac, val_frac, test_frac = len(train_idx)/len(full_idx), len(val_idx)/len(full_idx), len(test_idx)/len(full_idx)\n",
    "    torch.manual_seed(1103)\n",
    "    train_idx, val_idx, test_idx = torch.utils.data.random_split(full_idx, [int(train_frac*len(full_idx)), int(val_frac*len(full_idx)), int(test_frac*len(full_idx))])\n",
    "    train_idx = torch.sort(torch.tensor(train_idx.indices))[0]\n",
    "    val_idx = torch.sort(torch.tensor(val_idx.indices))[0]\n",
    "    test_idx = torch.sort(torch.tensor(test_idx.indices))[0]\n",
    "    split_idx = {'train': train_idx, 'valid': val_idx, 'test': test_idx}\n",
    "    return split_idx\n",
    "\n",
    "def mem_report():\n",
    "    if device.type == 'cuda':\n",
    "        GPUs = GPUtil.getGPUs()\n",
    "        for i, gpu in enumerate(GPUs):\n",
    "            print('GPU {:d} ... Mem Free: {:.0f}MB / {:.0f}MB | Utilization {:3.0f}%'.format(\n",
    "                i, gpu.memoryFree, gpu.memoryTotal, gpu.memoryUtil*100))\n",
    "    else:\n",
    "        print(\"CPU RAM Free: \"\n",
    "              + humanize.naturalsize(psutil.virtual_memory().available))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, args):\n",
    "    model.train()\n",
    "    # No batch\n",
    "    out = model(data.x, data.edge_index)\n",
    "    label = F.one_hot(data.y.squeeze(1), num_classes = dataset.num_classes).to(torch.float)\n",
    "    loss = nn.MSELoss()(out, label)*dataset.num_classes\n",
    "    loss.backward()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def train_SVI(model, data, args):\n",
    "    model.train()\n",
    "    model.layers_Xtilde = []\n",
    "    model.layers_grad = []\n",
    "    model.on_training = True # IF false, then SVI is NOT used\n",
    "    out = model(data.x, data.edge_index)\n",
    "    label = F.one_hot(data.y.squeeze(1), num_classes = dataset.num_classes).to(torch.float)\n",
    "    loss = nn.MSELoss()(out, label)*dataset.num_classes\n",
    "    model.turn_on_off_grad(on = False) \n",
    "    loss.backward(retain_graph = True)  # To get grad of L w.r.t. X_{l+1} for all layers at once\n",
    "    model.turn_on_off_grad(on = True)\n",
    "    for Xlplus1, Xlplus1grad in zip(model.layers_Xtilde, model.layers_grad):\n",
    "        Xlplus1grad = Xlplus1grad.grad.detach().to(device)\n",
    "        loss_tilde = (Xlplus1*Xlplus1grad).sum()\n",
    "        loss_tilde.backward(retain_graph=True)\n",
    "        model.turn_off_grad_during_SVI()\n",
    "    model.on_training = False\n",
    "    model.turn_on_off_grad(on = True) \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, FC=False):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.FC = FC\n",
    "        if self.FC:\n",
    "            self.convs.append(nn.Linear(in_channels, hidden_channels))\n",
    "        else:\n",
    "            self.convs.append(GCNConv(in_channels, hidden_channels, cached=True))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            if self.FC:\n",
    "                self.convs.append(nn.Linear(hidden_channels, hidden_channels))\n",
    "            else:\n",
    "                self.convs.append(\n",
    "                    GCNConv(hidden_channels, hidden_channels, cached=True))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        if self.FC:\n",
    "            self.convs.append(nn.Linear(hidden_channels, out_channels))\n",
    "        else:\n",
    "            self.convs.append(GCNConv(hidden_channels, out_channels, cached=True))\n",
    "        \n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if self.FC:\n",
    "                x = conv(x)\n",
    "            else:\n",
    "                x = conv(x, edge_index)\n",
    "            if i < len(self.convs)-1:\n",
    "                x = self.bns[i](x)\n",
    "                x = F.relu(x)\n",
    "            else:\n",
    "                x = x.softmax(dim=1)\n",
    "        return x\n",
    "\n",
    "class GNN_SVI(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, FC=False):\n",
    "        super(GNN_SVI, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.FC = FC\n",
    "        if self.FC:\n",
    "            self.convs.append(nn.Linear(in_channels, hidden_channels))\n",
    "        else:\n",
    "            self.convs.append(GCNConv(in_channels, hidden_channels, cached=True))\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            if self.FC:\n",
    "                self.convs.append(nn.Linear(hidden_channels, hidden_channels))\n",
    "            else:\n",
    "                self.convs.append(\n",
    "                    GCNConv(hidden_channels, hidden_channels, cached=True))\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "        if self.FC:\n",
    "            self.convs.append(nn.Linear(hidden_channels, out_channels))\n",
    "        else:\n",
    "            self.convs.append(\n",
    "                GCNConv(hidden_channels, out_channels, cached=True))\n",
    "        #### New lines for SVI ####\n",
    "        self.layers_Xtilde = []\n",
    "        self.layers_grad = []\n",
    "        self.on_training = True\n",
    "        #### End #####\n",
    "\n",
    "    #### New lines for SVI ####\n",
    "    # Avoid gradient accumulation\n",
    "    def turn_on_off_grad(self, on = True):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = on\n",
    "            \n",
    "    def turn_off_grad_during_SVI(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad:\n",
    "                if param.grad is not None and param.grad.sum() != 0:\n",
    "                    # Turn off since its SVI update direction is already computed\n",
    "                    param.requires_grad = False\n",
    "    #### End #####\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            if self.FC:\n",
    "                x = conv(x)\n",
    "            else:\n",
    "                x = conv(x, edge_index)\n",
    "            if self.on_training and i == len(self.convs)-1:\n",
    "                # No BN, last layer\n",
    "                self.layers_Xtilde.append(x)\n",
    "            if i < len(self.convs)-1:\n",
    "                x = self.bns[i](x)\n",
    "                if self.on_training:\n",
    "                    self.layers_Xtilde.append(x)\n",
    "                x = F.relu(x)\n",
    "            else:\n",
    "                x = x.softmax(dim=1)\n",
    "            if self.on_training:\n",
    "                x.retain_grad()  # To get the gradient with respect to output\n",
    "                self.layers_grad.append(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr, optim_name = 0.001, 'SGD'\n",
    "SVI_ls = [True, False]  # If False, use ordinary SGD or Adam\n",
    "SVI_pause = [False, False]  # If True, only SVI to warm start\n",
    "FC = False  # If use fully-connected layers. Default False. NOTE, it can be removed when introducing this as an example publically\n",
    "hidden_channels_ls = [256]\n",
    "num_runs = 3\n",
    "num_epochs = 1000\n",
    "epoch_stop_SVI = int(num_epochs/10) # When to stop SVI as warm start\n",
    "\n",
    "result_ls = []\n",
    "if __name__ == \"__main__\":\n",
    "    for use_SVI, pause_SVI in zip(SVI_ls, SVI_pause):\n",
    "        for hidden_channels in hidden_channels_ls:\n",
    "            result_dict = {'SVI-SGD': [],\n",
    "                            'SVI_warmstart-SGD': [],\n",
    "                            'SGD': [],\n",
    "                            'SVI-Adam': [],\n",
    "                            'SVI_warmstart-Adam': [],\n",
    "                            'Adam': []}\n",
    "            parser = argparse.ArgumentParser(\n",
    "                description='OGBN-Arxiv (GNN)')\n",
    "            parser.add_argument('--log_steps', type=int, default=1)\n",
    "            parser.add_argument('--num_layers', type=int, default=4)\n",
    "            parser.add_argument('--lr', type=float, default=lr)\n",
    "            parser.add_argument('--momentum', type=float, default=0.95)\n",
    "            parser.add_argument('--epochs', type=int, default=num_epochs)\n",
    "            parser.add_argument('--batch', type=int, default=1)\n",
    "            parser.add_argument('--runs', type=int, default=num_runs)\n",
    "            parser.add_argument('--SVI', type=bool, default=use_SVI)\n",
    "            parser.add_argument(\n",
    "                '--optimizer', type=str, default=optim_name)\n",
    "            args = parser.parse_args(args=[])\n",
    "            args.FC = FC # If use fully-connected nets instead of GCN layers\n",
    "            # args.hidden_channels = 128 if args.num_layers >= 3 else 1000\n",
    "            args.hidden_channels = hidden_channels\n",
    "            print(args)\n",
    "            dataset = PygNodePropPredDataset(name='ogbn-arxiv',\n",
    "                                            transform=T.ToSparseTensor())\n",
    "            data = dataset[0]\n",
    "            split_idx = dataset.get_idx_split()\n",
    "            split_idx = shuffle_split_idx(split_idx)\n",
    "\n",
    "            data = dataset[0]\n",
    "            data = data.to(device)\n",
    "            data.adj_t = data.adj_t.to_symmetric()\n",
    "            data_train=slide_idx(data, split_idx['train'])\n",
    "            logger = Logger(args.runs, args)\n",
    "            results_over_runs = {}\n",
    "            for run in range(args.runs):\n",
    "                accu_at_run = []\n",
    "                args.SVI = use_SVI\n",
    "                torch.manual_seed(1103 + run)\n",
    "                if args.SVI:\n",
    "                    model = GNN_SVI(data.num_features, args.hidden_channels,\n",
    "                                    dataset.num_classes, args.num_layers, args.FC).to(device)\n",
    "                else:\n",
    "                    model = GNN(data.num_features, args.hidden_channels,\n",
    "                                dataset.num_classes, args.num_layers, args.FC).to(device)\n",
    "                evaluator = Evaluator(name='ogbn-arxiv')\n",
    "                if args.optimizer == 'SGD':\n",
    "                    optimizer = torch.optim.SGD(\n",
    "                        model.parameters(), lr=args.lr, momentum=args.momentum, nesterov=True)\n",
    "                else:\n",
    "                    optimizer = torch.optim.Adam(\n",
    "                        model.parameters(), lr=args.lr)\n",
    "                for epoch in range(1, 1 + args.epochs):\n",
    "                    if device.type == 'cuda':\n",
    "                        # Useful to avoid GPU allocation excess\n",
    "                        torch.cuda.empty_cache()\n",
    "                    # print(f\"LR is {optimizer.param_groups[0]['lr']}\")\n",
    "                    if epoch == epoch_stop_SVI + 1 and pause_SVI:\n",
    "                        # Reinitialize optimizer to avoid gradient issue\n",
    "                        args.SVI = False\n",
    "                        sdict = model.state_dict()\n",
    "                        print(\n",
    "                            '############ Pause SVI from now on ############')\n",
    "                        model = GNN(data.num_features, args.hidden_channels,\n",
    "                            dataset.num_classes, args.num_layers,\n",
    "                            args.dropout, args.FC).to(device)\n",
    "                        model.load_state_dict(sdict)\n",
    "                        model = model.to(device)\n",
    "                        if args.optimizer == 'SGD':\n",
    "                            optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, \n",
    "                                                        momentum=args.momentum, \n",
    "                                                        nesterov=True)\n",
    "                        else:\n",
    "                            optimizer = torch.optim.Adam(\n",
    "                                model.parameters(), lr=args.lr)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = train_SVI(model, data_train, args) if args.SVI else train(model, data_train, args)\n",
    "                    optimizer.step()\n",
    "                    # print('Testing')\n",
    "                    if epoch % args.log_steps == 0:\n",
    "                        print('Testing')\n",
    "                        if args.SVI:\n",
    "                            print(f'SVI-{args.optimizer} training at epoch {epoch}')\n",
    "                        else:\n",
    "                            print(f'{args.optimizer} training at epoch {epoch}')\n",
    "                        result = test(model, data, split_idx, evaluator)                       \n",
    "                        mem_report()    \n",
    "                        logger.add_result(run, result)\n",
    "                        train_acc, valid_acc, test_acc = result\n",
    "                        accu_at_run += [[train_acc, valid_acc, test_acc]]\n",
    "                        print(f'Run: {run + 1:02d}, '\n",
    "                                f'Epoch: {epoch:02d}, '\n",
    "                                f'Loss: {loss:.4f}, '\n",
    "                                f'Train: {100 * train_acc:.2f}%, '\n",
    "                                f'Valid: {100 * valid_acc:.2f}% '\n",
    "                                f'Test: {100 * test_acc:.2f}%')\n",
    "                # Running np.array(accu_at_run) would make it into Epoch-by-3 matrices, but doing so causes .json saving error so I just use the list version\n",
    "                results_over_runs[f'lr={args.lr}@Run{run+1}'] = accu_at_run\n",
    "                logger.print_statistics(run)\n",
    "                # Save results\n",
    "                if use_SVI:\n",
    "                    SVI_prefix = 'SVI_warmstart-' if pause_SVI else 'SVI-'\n",
    "                else:\n",
    "                    SVI_prefix = ''\n",
    "                key = f'{SVI_prefix}{optim_name}'\n",
    "                fc_use = '-FC' if args.FC else ''\n",
    "                key_save = f'{SVI_prefix}{optim_name}-{args.num_layers}layers-{args.hidden_channels}nodes-{args.lr}LR{fc_use}_correct_split_1'\n",
    "                # logger.pickle(key_save) # Save it to file, but need not now because only one run.\n",
    "                result_dict[key].append(results_over_runs)\n",
    "                with open(f\"{key_save}_loss_together_SVI_only_shuffle_{num_epochs}.json\", \"w\") as outfile:\n",
    "                    json.dump(result_dict, outfile)\n",
    "            logger.print_statistics()\n",
    "            result_ls.append(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for logger in result_ls:\n",
    "    logger.print_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
